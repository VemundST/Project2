{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import log_reg_functions as lrf\n",
    "import loaddata as ld\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = ld.load_data(scaler='minmax')\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size=0.25, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost [[0.94143818]] & Log loss 13.129738270293714 & Cost test [[0.93790982]]\n",
      "Cost [[0.92926369]] & Log loss 12.992726962724555 & Cost test [[0.92590063]]\n",
      "Cost [[0.91752353]] & Log loss 12.87855087308359 & Cost test [[0.91431534]]\n",
      "Cost [[0.90620078]] & Log loss 12.801844308623712 & Cost test [[0.90313766]]\n",
      "Cost [[0.89527908]] & Log loss 12.751257044083685 & Cost test [[0.89235191]]\n",
      "Cost [[0.88474262]] & Log loss 12.720309211366699 & Cost test [[0.88194292]]\n",
      "Cost [[0.87457618]] & Log loss 12.628931162438557 & Cost test [[0.87189606]]\n",
      "Cost [[0.86476505]] & Log loss 12.558774173354308 & Cost test [[0.86219723]]\n",
      "Cost [[0.85529505]] & Log loss 12.472271128742516 & Cost test [[0.85283281]]\n",
      "Cost [[0.84615251]] & Log loss 12.333580602962162 & Cost test [[0.84378966]]\n",
      "Cost [[0.83732427]] & Log loss 12.132850350038273 & Cost test [[0.83505513]]\n",
      "Cost [[0.82879763]] & Log loss 11.928910369553465 & Cost test [[0.82661703]]\n",
      "Cost [[0.8205604]] & Log loss 11.739641873536927 & Cost test [[0.8184636]]\n",
      "Cost [[0.81260085]] & Log loss 11.58464711659632 & Cost test [[0.81058355]]\n",
      "Cost [[0.80490769]] & Log loss 11.449212789896094 & Cost test [[0.802966]]\n",
      "Cost [[0.79747009]] & Log loss 11.406826436031363 & Cost test [[0.79560048]]\n",
      "Cost [[0.79027765]] & Log loss 11.333390012107378 & Cost test [[0.78847696]]\n",
      "Cost [[0.78332039]] & Log loss 11.294250563018936 & Cost test [[0.78158578]]\n",
      "Cost [[0.77658875]] & Log loss 11.21105948331033 & Cost test [[0.77491766]]\n",
      "Cost [[0.77007355]] & Log loss 11.056111197888933 & Cost test [[0.76846373]]\n",
      "Cost [[0.763766]] & Log loss 10.912536373488384 & Cost test [[0.76221545]]\n",
      "Cost [[0.75765771]] & Log loss 10.783646975011864 & Cost test [[0.75616465]]\n",
      "Cost [[0.75174061]] & Log loss 10.708577804395896 & Cost test [[0.75030349]]\n",
      "Cost [[0.74600702]] & Log loss 10.654720399319975 & Cost test [[0.74462448]]\n",
      "Cost [[0.74044956]] & Log loss 10.597592853708163 & Cost test [[0.73912043]]\n",
      "Cost [[0.73506122]] & Log loss 10.537213756168141 & Cost test [[0.73378447]]\n",
      "Cost [[0.72983526]] & Log loss 10.48991522077169 & Cost test [[0.72861002]]\n",
      "Cost [[0.72476525]] & Log loss 10.437690562387756 & Cost test [[0.7235908]]\n",
      "Cost [[0.71984508]] & Log loss 10.414850697307642 & Cost test [[0.71872079]]\n",
      "Cost [[0.71506889]] & Log loss 10.388731397387794 & Cost test [[0.71399424]]\n",
      "Cost [[0.71043108]] & Log loss 10.354485541223385 & Cost test [[0.70940565]]\n",
      "Cost [[0.70592633]] & Log loss 10.254934464531472 & Cost test [[0.70494979]]\n",
      "Cost [[0.70154954]] & Log loss 10.065652027059167 & Cost test [[0.70062163]]\n",
      "Cost [[0.69729587]] & Log loss 9.979288397005014 & Cost test [[0.69641637]]\n",
      "Cost [[0.69316066]] & Log loss 9.8976882396211 & Cost test [[0.69232946]]\n",
      "Cost [[0.68913951]] & Log loss 9.865065835844838 & Cost test [[0.6883565]]\n",
      "Cost [[0.6852282]] & Log loss 9.830801391072745 & Cost test [[0.68449332]]\n",
      "Cost [[0.6814227]] & Log loss 9.817758006144546 & Cost test [[0.68073595]]\n",
      "Cost [[0.67771916]] & Log loss 9.798178987296481 & Cost test [[0.67708055]]\n",
      "Cost [[0.67411393]] & Log loss 9.768808135448424 & Cost test [[0.6735235]]\n",
      "Cost [[0.6706035]] & Log loss 9.670894452600418 & Cost test [[0.67006131]]\n",
      "Cost [[0.66718454]] & Log loss 9.561588720123884 & Cost test [[0.66669066]]\n",
      "Cost [[0.66385384]] & Log loss 9.455492715208266 & Cost test [[0.66340837]]\n",
      "Cost [[0.66060837]] & Log loss 9.383670449368582 & Cost test [[0.66021139]]\n",
      "Cost [[0.6574452]] & Log loss 9.334701990064778 & Cost test [[0.65709683]]\n",
      "Cost [[0.65436156]] & Log loss 9.30533113821672 & Cost test [[0.6540619]]\n",
      "Cost [[0.65135479]] & Log loss 9.266182394824435 & Cost test [[0.65110395]]\n",
      "Cost [[0.64842233]] & Log loss 9.174776462984765 & Cost test [[0.64822041]]\n",
      "Cost [[0.64556175]] & Log loss 9.07034108767266 & Cost test [[0.64540886]]\n",
      "Cost [[0.64277071]] & Log loss 8.951187756373077 & Cost test [[0.64266696]]\n",
      "Cost [[0.64004699]] & Log loss 8.85972141155843 & Cost test [[0.63999247]]\n",
      "Cost [[0.63738845]] & Log loss 8.80585471217867 & Cost test [[0.63738324]]\n",
      "Cost [[0.63479302]] & Log loss 8.763421886794728 & Cost test [[0.63483721]]\n",
      "Cost [[0.63225875]] & Log loss 8.722640396710455 & Cost test [[0.63235241]]\n",
      "Cost [[0.62978374]] & Log loss 8.703066025014312 & Cost test [[0.62992693]]\n",
      "Cost [[0.62736618]] & Log loss 8.655753548162098 & Cost test [[0.62755896]]\n",
      "Cost [[0.62500433]] & Log loss 8.628043325917554 & Cost test [[0.62524674]]\n",
      "Cost [[0.62269652]] & Log loss 8.616641981985182 & Cost test [[0.6229886]]\n",
      "Cost [[0.62044113]] & Log loss 8.613395077208896 & Cost test [[0.6207829]]\n",
      "Cost [[0.61823662]] & Log loss 8.613413665816582 & Cost test [[0.61862809]]\n",
      "Cost [[0.6160815]] & Log loss 8.606887326200559 & Cost test [[0.61652268]]\n",
      "Cost [[0.61397434]] & Log loss 8.598723592740631 & Cost test [[0.61446521]]\n",
      "Cost [[0.61191374]] & Log loss 8.59545809935666 & Cost test [[0.61245429]]\n",
      "Cost [[0.60989838]] & Log loss 8.574218450905082 & Cost test [[0.61048857]]\n",
      "Cost [[0.60792698]] & Log loss 8.56442197075317 & Cost test [[0.60856677]]\n",
      "Cost [[0.60599828]] & Log loss 8.551355350065363 & Cost test [[0.60668763]]\n",
      "Cost [[0.6041111]] & Log loss 8.546461757141326 & Cost test [[0.60484994]]\n",
      "Cost [[0.60226428]] & Log loss 8.538298023681397 & Cost test [[0.60305254]]\n",
      "Cost [[0.6004567]] & Log loss 8.525231402993592 & Cost test [[0.6012943]]\n",
      "Cost [[0.59868728]] & Log loss 8.520333162917636 & Cost test [[0.59957412]]\n",
      "Cost [[0.59695498]] & Log loss 8.45828878862218 & Cost test [[0.59789097]]\n",
      "Cost [[0.59525878]] & Log loss 8.332632353466183 & Cost test [[0.59624381]]\n",
      "Cost [[0.59359772]] & Log loss 8.236332828702478 & Cost test [[0.59463166]]\n",
      "Cost [[0.59197083]] & Log loss 8.16778535055061 & Cost test [[0.59305357]]\n",
      "Cost [[0.59037721]] & Log loss 8.13841449870255 & Cost test [[0.59150861]]\n",
      "Cost [[0.58881597]] & Log loss 8.128613371398716 & Cost test [[0.58999588]]\n",
      "Cost [[0.58728625]] & Log loss 8.117184144554818 & Cost test [[0.58851453]]\n",
      "Cost [[0.58578722]] & Log loss 8.110657804938796 & Cost test [[0.5870637]]\n",
      "Cost [[0.58431806]] & Log loss 8.100884560546488 & Cost test [[0.58564258]]\n",
      "Cost [[0.58287799]] & Log loss 8.071471884331139 & Cost test [[0.58425039]]\n",
      "Cost [[0.58146626]] & Log loss 8.04695744819175 & Cost test [[0.58288636]]\n",
      "Cost [[0.58008212]] & Log loss 8.045338642955528 & Cost test [[0.58154974]]\n",
      "Cost [[0.57872487]] & Log loss 8.029011176035672 & Cost test [[0.58023981]]\n",
      "Cost [[0.5773938]] & Log loss 8.027387723647527 & Cost test [[0.57895587]]\n",
      "Cost [[0.57608824]] & Log loss 8.012697650571578 & Cost test [[0.57769725]]\n",
      "Cost [[0.57480754]] & Log loss 8.001259129423836 & Cost test [[0.57646328]]\n",
      "Cost [[0.57355106]] & Log loss 7.999631029883771 & Cost test [[0.57525333]]\n",
      "Cost [[0.57231819]] & Log loss 7.991476590727686 & Cost test [[0.57406677]]\n",
      "Cost [[0.57110833]] & Log loss 7.980042716731864 & Cost test [[0.572903]]\n",
      "Cost [[0.56992089]] & Log loss 7.97840997003988 & Cost test [[0.57176144]]\n",
      "Cost [[0.56875532]] & Log loss 7.971874336120015 & Cost test [[0.57064152]]\n",
      "Cost [[0.56761106]] & Log loss 7.965343349352072 & Cost test [[0.56954269]]\n",
      "Cost [[0.56648757]] & Log loss 7.95881236258413 & Cost test [[0.5684644]]\n",
      "Cost [[0.56538435]] & Log loss 7.953904828204331 & Cost test [[0.56740615]]\n",
      "Cost [[0.56430088]] & Log loss 7.9571842630440655 & Cost test [[0.56636742]]\n",
      "Cost [[0.56323669]] & Log loss 7.950639334820359 & Cost test [[0.56534772]]\n",
      "Cost [[0.56219129]] & Log loss 7.944089759444731 & Cost test [[0.56434658]]\n",
      "Cost [[0.56116422]] & Log loss 7.937554125524868 & Cost test [[0.56336353]]\n",
      "Cost [[0.56015503]] & Log loss 7.931018491605005 & Cost test [[0.56239812]]\n",
      "Cost [[0.5591633]] & Log loss 7.931013844453083 & Cost test [[0.56144992]]\n",
      "Cost [[0.55818859]] & Log loss 7.922826875233549 & Cost test [[0.5605185]]\n",
      "Cost [[0.55723049]] & Log loss 7.904838778710179 & Cost test [[0.55960344]]\n",
      "Cost [[0.55628861]] & Log loss 7.901577932478129 & Cost test [[0.55870436]]\n",
      "Cost [[0.55536255]] & Log loss 7.890144058482309 & Cost test [[0.55782084]]\n",
      "Cost [[0.55445193]] & Log loss 7.888515958942244 & Cost test [[0.55695253]]\n",
      "Cost [[0.5535564]] & Log loss 7.878700890182644 & Cost test [[0.55609904]]\n",
      "Cost [[0.55267559]] & Log loss 7.875435396798674 & Cost test [[0.55526003]]\n",
      "Cost [[0.55180915]] & Log loss 7.875435396798674 & Cost test [[0.55443514]]\n",
      "Cost [[0.55095675]] & Log loss 7.875440043950595 & Cost test [[0.55362404]]\n",
      "Cost [[0.55011807]] & Log loss 7.862359481807025 & Cost test [[0.5528264]]\n",
      "Cost [[0.54929277]] & Log loss 7.85908004696729 & Cost test [[0.5520419]]\n",
      "Cost [[0.54848056]] & Log loss 7.844357443827891 & Cost test [[0.55127023]]\n",
      "Cost [[0.54768113]] & Log loss 7.841082656140077 & Cost test [[0.5505111]]\n",
      "Cost [[0.54689419]] & Log loss 7.831276881684322 & Cost test [[0.5497642]]\n",
      "Cost [[0.54611945]] & Log loss 7.829639487840415 & Cost test [[0.54902925]]\n",
      "Cost [[0.54535664]] & Log loss 7.823103853920551 & Cost test [[0.54830598]]\n",
      "Cost [[0.54460549]] & Log loss 7.813298079464795 & Cost test [[0.54759412]]\n",
      "Cost [[0.54386573]] & Log loss 7.811656038468966 & Cost test [[0.54689341]]\n",
      "Cost [[0.54313712]] & Log loss 7.808395192236916 & Cost test [[0.5462036]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost [[0.5424194]] & Log loss 7.806771739848773 & Cost test [[0.54552443]]\n",
      "Cost [[0.54171234]] & Log loss 7.8018734997728165 & Cost test [[0.54485567]]\n",
      "Cost [[0.5410157]] & Log loss 7.79206772531706 & Cost test [[0.54419708]]\n",
      "Cost [[0.54032926]] & Log loss 7.788811526236931 & Cost test [[0.54354844]]\n",
      "Cost [[0.53965279]] & Log loss 7.7855413857010385 & Cost test [[0.54290953]]\n",
      "Cost [[0.53898607]] & Log loss 7.787183426696867 & Cost test [[0.54228014]]\n",
      "Cost [[0.53832891]] & Log loss 7.780647792777002 & Cost test [[0.54166005]]\n",
      "Cost [[0.53768109]] & Log loss 7.770837371169324 & Cost test [[0.54104906]]\n",
      "Cost [[0.53704241]] & Log loss 7.764306384401381 & Cost test [[0.54044698]]\n",
      "Cost [[0.53641269]] & Log loss 7.756133356637611 & Cost test [[0.53985362]]\n",
      "Cost [[0.53579174]] & Log loss 7.749597722717748 & Cost test [[0.53926878]]\n",
      "Cost [[0.53517938]] & Log loss 7.744694835489869 & Cost test [[0.53869229]]\n",
      "Cost [[0.53457542]] & Log loss 7.744694835489869 & Cost test [[0.53812397]]\n",
      "Cost [[0.5339797]] & Log loss 7.7463322293337775 & Cost test [[0.53756364]]\n",
      "Cost [[0.53339204]] & Log loss 7.738154554418085 & Cost test [[0.53701115]]\n",
      "Cost [[0.53281228]] & Log loss 7.738145260114243 & Cost test [[0.53646633]]\n",
      "Cost [[0.53224027]] & Log loss 7.729953643742787 & Cost test [[0.53592902]]\n",
      "Cost [[0.53167585]] & Log loss 7.736493924814571 & Cost test [[0.53539907]]\n",
      "Cost [[0.53111887]] & Log loss 7.73648463051073 & Cost test [[0.53487632]]\n",
      "Cost [[0.53056917]] & Log loss 7.736470689054966 & Cost test [[0.53436064]]\n",
      "Cost [[0.53002663]] & Log loss 7.733205195670994 & Cost test [[0.53385187]]\n",
      "Cost [[0.52949109]] & Log loss 7.728293014139274 & Cost test [[0.53334989]]\n",
      "Cost [[0.52896242]] & Log loss 7.7201153392235815 & Cost test [[0.53285455]]\n",
      "Cost [[0.52844049]] & Log loss 7.713579705303719 & Cost test [[0.53236573]]\n",
      "Cost [[0.52792518]] & Log loss 7.705406677539948 & Cost test [[0.53188329]]\n",
      "Cost [[0.52741634]] & Log loss 7.698866396468162 & Cost test [[0.53140712]]\n",
      "Cost [[0.52691387]] & Log loss 7.7005037903120686 & Cost test [[0.53093708]]\n",
      "Cost [[0.52641764]] & Log loss 7.70376928369604 & Cost test [[0.53047308]]\n",
      "Cost [[0.52592754]] & Log loss 7.702131889852134 & Cost test [[0.53001497]]\n",
      "Cost [[0.52544345]] & Log loss 7.69559625593227 & Cost test [[0.52956267]]\n",
      "Cost [[0.52496526]] & Log loss 7.687423228168499 & Cost test [[0.52911605]]\n",
      "Cost [[0.52449287]] & Log loss 7.687423228168499 & Cost test [[0.52867501]]\n",
      "Cost [[0.52402616]] & Log loss 7.6808829470967135 & Cost test [[0.52823944]]\n",
      "Cost [[0.52356504]] & Log loss 7.67434266602493 & Cost test [[0.52780925]]\n",
      "Cost [[0.5231094]] & Log loss 7.667802384953144 & Cost test [[0.52738433]]\n",
      "Cost [[0.52265916]] & Log loss 7.6612574567294365 & Cost test [[0.52696459]]\n",
      "Cost [[0.5222142]] & Log loss 7.661252809577515 & Cost test [[0.52654994]]\n",
      "Cost [[0.52177445]] & Log loss 7.656349922349638 & Cost test [[0.52614027]]\n",
      "Cost [[0.5213398]] & Log loss 7.656345275197716 & Cost test [[0.52573551]]\n",
      "Cost [[0.52091018]] & Log loss 7.649809641277855 & Cost test [[0.52533557]]\n",
      "Cost [[0.52048548]] & Log loss 7.643274007357991 & Cost test [[0.52494035]]\n",
      "Cost [[0.52006564]] & Log loss 7.64001316112594 & Cost test [[0.52454978]]\n",
      "Cost [[0.51965056]] & Log loss 7.63675231489389 & Cost test [[0.52416377]]\n",
      "Cost [[0.51924017]] & Log loss 7.628588581433961 & Cost test [[0.52378225]]\n",
      "Cost [[0.51883438]] & Log loss 7.6220575946660185 & Cost test [[0.52340514]]\n",
      "Cost [[0.51843312]] & Log loss 7.620424847974033 & Cost test [[0.52303235]]\n",
      "Cost [[0.51803631]] & Log loss 7.620424847974033 & Cost test [[0.52266383]]\n",
      "Cost [[0.51764389]] & Log loss 7.615526607898077 & Cost test [[0.52229949]]\n",
      "Cost [[0.51725577]] & Log loss 7.617164001741982 & Cost test [[0.52193926]]\n",
      "Cost [[0.5168719]] & Log loss 7.61880139558589 & Cost test [[0.52158308]]\n",
      "Cost [[0.51649219]] & Log loss 7.617168648893903 & Cost test [[0.52123087]]\n",
      "Cost [[0.51611658]] & Log loss 7.607372168741991 & Cost test [[0.52088258]]\n",
      "Cost [[0.51574501]] & Log loss 7.592663507058355 & Cost test [[0.52053813]]\n",
      "Cost [[0.51537741]] & Log loss 7.592672801362198 & Cost test [[0.52019747]]\n",
      "Cost [[0.51501372]] & Log loss 7.584499773598427 & Cost test [[0.51986052]]\n",
      "Cost [[0.51465388]] & Log loss 7.5779641396785635 & Cost test [[0.51952724]]\n",
      "Cost [[0.51429783]] & Log loss 7.574703293446514 & Cost test [[0.51919756]]\n",
      "Cost [[0.5139455]] & Log loss 7.568163012374728 & Cost test [[0.51887142]]\n",
      "Cost [[0.51359684]] & Log loss 7.566530265682743 & Cost test [[0.51854877]]\n",
      "Cost [[0.5132518]] & Log loss 7.564897518990758 & Cost test [[0.51822956]]\n",
      "Cost [[0.51291032]] & Log loss 7.5616227313029425 & Cost test [[0.51791372]]\n",
      "Cost [[0.51257234]] & Log loss 7.563250830843007 & Cost test [[0.5176012]]\n",
      "Cost [[0.51223781]] & Log loss 7.5550499201677095 & Cost test [[0.51729196]]\n",
      "Cost [[0.51190668]] & Log loss 7.548504991944002 & Cost test [[0.51698593]]\n",
      "Cost [[0.51157891]] & Log loss 7.540331964180232 & Cost test [[0.51668308]]\n",
      "Cost [[0.51125442]] & Log loss 7.548495697640161 & Cost test [[0.51638336]]\n",
      "Cost [[0.51093319]] & Log loss 7.528860912969041 & Cost test [[0.5160867]]\n",
      "Cost [[0.51061516]] & Log loss 7.522306690441494 & Cost test [[0.51579308]]\n",
      "Cost [[0.51030029]] & Log loss 7.512491621681895 & Cost test [[0.51550244]]\n",
      "Cost [[0.50998852]] & Log loss 7.50756549869441 & Cost test [[0.51521473]]\n",
      "Cost [[0.50967982]] & Log loss 7.497759724238654 & Cost test [[0.51492992]]\n",
      "Cost [[0.50937414]] & Log loss 7.489586696474883 & Cost test [[0.51464796]]\n",
      "Cost [[0.50907143]] & Log loss 7.486316555938991 & Cost test [[0.5143688]]\n",
      "Cost [[0.50877166]] & Log loss 7.471607894255356 & Cost test [[0.51409241]]\n",
      "Cost [[0.50847478]] & Log loss 7.4634395136435066 & Cost test [[0.51381875]]\n",
      "Cost [[0.50818075]] & Log loss 7.4552664858797355 & Cost test [[0.51354777]]\n",
      "Cost [[0.50788953]] & Log loss 7.453629092035828 & Cost test [[0.51327944]]\n",
      "Cost [[0.50760109]] & Log loss 7.447084163812122 & Cost test [[0.51301371]]\n",
      "Cost [[0.50731537]] & Log loss 7.43890184174451 & Cost test [[0.51275056]]\n",
      "Cost [[0.50703235]] & Log loss 7.438897194592587 & Cost test [[0.51248994]]\n",
      "Cost [[0.50675199]] & Log loss 7.435613112600931 & Cost test [[0.51223181]]\n",
      "Cost [[0.50647425]] & Log loss 7.429063537225304 & Cost test [[0.51197614]]\n",
      "Cost [[0.50619909]] & Log loss 7.424146708541662 & Cost test [[0.51172291]]\n",
      "Cost [[0.50592648]] & Log loss 7.424142061389741 & Cost test [[0.51147206]]\n",
      "Cost [[0.50565639]] & Log loss 7.424128119933978 & Cost test [[0.51122357]]\n",
      "Cost [[0.50538878]] & Log loss 7.424118825630135 & Cost test [[0.5109774]]\n",
      "Cost [[0.50512362]] & Log loss 7.419201996946493 & Cost test [[0.51073353]]\n",
      "Cost [[0.50486087]] & Log loss 7.417555308798745 & Cost test [[0.51049192]]\n",
      "Cost [[0.5046005]] & Log loss 7.417555308798745 & Cost test [[0.51025254]]\n",
      "Cost [[0.50434249]] & Log loss 7.417550661646823 & Cost test [[0.51001536]]\n",
      "Cost [[0.5040868]] & Log loss 7.415917914954837 & Cost test [[0.50978034]]\n",
      "Cost [[0.50383339]] & Log loss 7.415917914954837 & Cost test [[0.50954746]]\n",
      "Cost [[0.50358224]] & Log loss 7.412647774418946 & Cost test [[0.5093167]]\n",
      "Cost [[0.50333333]] & Log loss 7.412647774418946 & Cost test [[0.50908801]]\n",
      "Cost [[0.50308661]] & Log loss 7.412643127267024 & Cost test [[0.50886138]]\n",
      "Cost [[0.50284207]] & Log loss 7.412647774418946 & Cost test [[0.50863677]]\n",
      "Cost [[0.50259967]] & Log loss 7.414285168262852 & Cost test [[0.50841416]]\n",
      "Cost [[0.50235939]] & Log loss 7.41428052111093 & Cost test [[0.50819352]]\n",
      "Cost [[0.5021212]] & Log loss 7.41428052111093 & Cost test [[0.50797482]]\n",
      "Cost [[0.50188507]] & Log loss 7.409382281034974 & Cost test [[0.50775805]]\n",
      "Cost [[0.50165097]] & Log loss 7.411015027726959 & Cost test [[0.50754317]]\n",
      "Cost [[0.50141889]] & Log loss 7.409377633883053 & Cost test [[0.50733015]]\n",
      "Cost [[0.50118879]] & Log loss 7.409377633883053 & Cost test [[0.50711898]]\n",
      "Cost [[0.50096066]] & Log loss 7.409377633883053 & Cost test [[0.50690964]]\n",
      "Cost [[0.50073445]] & Log loss 7.407740240039146 & Cost test [[0.50670208]]\n",
      "Cost [[0.50051016]] & Log loss 7.409372986731131 & Cost test [[0.5064963]]\n",
      "Cost [[0.50028776]] & Log loss 7.409368339579211 & Cost test [[0.50629227]]\n",
      "Cost [[0.50006722]] & Log loss 7.40936833957921 & Cost test [[0.50608997]]\n",
      "Cost [[0.49984853]] & Log loss 7.406093551891397 & Cost test [[0.50588938]]\n",
      "Cost [[0.49963165]] & Log loss 7.404456158047489 & Cost test [[0.50569046]]\n",
      "Cost [[0.49941657]] & Log loss 7.401190664663518 & Cost test [[0.50549321]]\n",
      "Cost [[0.49920326]] & Log loss 7.401190664663518 & Cost test [[0.5052976]]\n",
      "Cost [[0.49899171]] & Log loss 7.401190664663518 & Cost test [[0.50510361]]\n",
      "Cost [[0.49878189]] & Log loss 7.401190664663518 & Cost test [[0.50491122]]\n",
      "Cost [[0.49857377]] & Log loss 7.399543976515768 & Cost test [[0.50472041]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost [[0.49836735]] & Log loss 7.397906582671862 & Cost test [[0.50453116]]\n",
      "Cost [[0.4981626]] & Log loss 7.397906582671862 & Cost test [[0.50434344]]\n",
      "Cost [[0.49795949]] & Log loss 7.3929990482920624 & Cost test [[0.50415725]]\n",
      "Cost [[0.49775802]] & Log loss 7.394631794984048 & Cost test [[0.50397256]]\n",
      "Cost [[0.49755816]] & Log loss 7.392994401140141 & Cost test [[0.50378936]]\n",
      "Cost [[0.49735988]] & Log loss 7.396264541676034 & Cost test [[0.50360761]]\n",
      "Cost [[0.49716318]] & Log loss 7.3929990482920624 & Cost test [[0.50342732]]\n",
      "Cost [[0.49696804]] & Log loss 7.38972890775617 & Cost test [[0.50324845]]\n",
      "Cost [[0.49677443]] & Log loss 7.388096161064183 & Cost test [[0.50307099]]\n",
      "Cost [[0.49658233]] & Log loss 7.386458767220277 & Cost test [[0.50289493]]\n",
      "Cost [[0.49639174]] & Log loss 7.386458767220277 & Cost test [[0.50272025]]\n",
      "Cost [[0.49620263]] & Log loss 7.386454120068356 & Cost test [[0.50254692]]\n",
      "Cost [[0.49601498]] & Log loss 7.3815465856885565 & Cost test [[0.50237494]]\n",
      "Cost [[0.49582878]] & Log loss 7.378271798000742 & Cost test [[0.50220429]]\n",
      "Cost [[0.49564401]] & Log loss 7.378267150848821 & Cost test [[0.50203495]]\n",
      "Cost [[0.49546066]] & Log loss 7.370075534477365 & Cost test [[0.5018669]]\n",
      "Cost [[0.4952787]] & Log loss 7.363521311949817 & Cost test [[0.50170013]]\n",
      "Cost [[0.49509813]] & Log loss 7.363516664797896 & Cost test [[0.50153463]]\n",
      "Cost [[0.49491892]] & Log loss 7.361879270953989 & Cost test [[0.50137038]]\n",
      "Cost [[0.49474107]] & Log loss 7.361879270953989 & Cost test [[0.50120736]]\n",
      "Cost [[0.49456455]] & Log loss 7.358609130418096 & Cost test [[0.50104557]]\n",
      "Cost [[0.49438935]] & Log loss 7.358604483266176 & Cost test [[0.50088497]]\n",
      "Cost [[0.49421545]] & Log loss 7.355334342730283 & Cost test [[0.50072557]]\n",
      "Cost [[0.49404285]] & Log loss 7.353692301734454 & Cost test [[0.50056735]]\n",
      "Cost [[0.49387152]] & Log loss 7.353687654582533 & Cost test [[0.50041029]]\n",
      "Cost [[0.49370146]] & Log loss 7.35041286689472 & Cost test [[0.50025439]]\n",
      "Cost [[0.49353264]] & Log loss 7.35041286689472 & Cost test [[0.50009961]]\n",
      "Cost [[0.49336505]] & Log loss 7.352050260738627 & Cost test [[0.49994597]]\n",
      "Cost [[0.49319869]] & Log loss 7.347142726358828 & Cost test [[0.49979343]]\n",
      "Cost [[0.49303353]] & Log loss 7.345505332514921 & Cost test [[0.49964199]]\n",
      "Cost [[0.49286956]] & Log loss 7.347133432054985 & Cost test [[0.49949163]]\n",
      "Cost [[0.49270678]] & Log loss 7.345500685362999 & Cost test [[0.49934235]]\n",
      "Cost [[0.49254516]] & Log loss 7.345500685362999 & Cost test [[0.49919412]]\n",
      "Cost [[0.4923847]] & Log loss 7.345500685362999 & Cost test [[0.49904695]]\n",
      "Cost [[0.49222537]] & Log loss 7.3487708258988915 & Cost test [[0.49890081]]\n",
      "Cost [[0.49206718]] & Log loss 7.3487708258988915 & Cost test [[0.49875569]]\n",
      "Cost [[0.4919101]] & Log loss 7.347138079206907 & Cost test [[0.49861158]]\n",
      "Cost [[0.49175413]] & Log loss 7.347138079206907 & Cost test [[0.49846848]]\n",
      "Cost [[0.49159925]] & Log loss 7.347138079206907 & Cost test [[0.49832637]]\n",
      "Cost [[0.49144544]] & Log loss 7.345500685362999 & Cost test [[0.49818523]]\n",
      "Cost [[0.49129271]] & Log loss 7.343867938671012 & Cost test [[0.49804506]]\n",
      "Cost [[0.49114104]] & Log loss 7.345500685362999 & Cost test [[0.49790585]]\n",
      "Cost [[0.49099041]] & Log loss 7.345500685362999 & Cost test [[0.49776758]]\n",
      "Cost [[0.49084082]] & Log loss 7.342225897675186 & Cost test [[0.49763025]]\n",
      "Cost [[0.49069225]] & Log loss 7.345496038211078 & Cost test [[0.49749384]]\n",
      "Cost [[0.49054469]] & Log loss 7.345496038211078 & Cost test [[0.49735835]]\n",
      "Cost [[0.49039813]] & Log loss 7.345496038211078 & Cost test [[0.49722376]]\n",
      "Cost [[0.49025257]] & Log loss 7.345491391059156 & Cost test [[0.49709006]]\n",
      "Cost [[0.49010798]] & Log loss 7.343853997215249 & Cost test [[0.49695725]]\n",
      "Cost [[0.48996437]] & Log loss 7.340579209527435 & Cost test [[0.49682531]]\n",
      "Cost [[0.48982172]] & Log loss 7.342216603371343 & Cost test [[0.49669424]]\n",
      "Cost [[0.48968001]] & Log loss 7.342216603371343 & Cost test [[0.49656402]]\n",
      "Cost [[0.48953925]] & Log loss 7.337309068991544 & Cost test [[0.49643465]]\n",
      "Cost [[0.48939942]] & Log loss 7.338941815683529 & Cost test [[0.49630612]]\n",
      "Cost [[0.4892605]] & Log loss 7.340579209527435 & Cost test [[0.49617841]]\n",
      "Cost [[0.4891225]] & Log loss 7.342211956219422 & Cost test [[0.49605152]]\n",
      "Cost [[0.4889854]] & Log loss 7.338941815683529 & Cost test [[0.49592544]]\n",
      "Cost [[0.48884919]] & Log loss 7.335671675147636 & Cost test [[0.49580016]]\n",
      "Cost [[0.48871386]] & Log loss 7.332396887459823 & Cost test [[0.49567567]]\n",
      "Cost [[0.48857941]] & Log loss 7.334024986999887 & Cost test [[0.49555197]]\n",
      "Cost [[0.48844582]] & Log loss 7.32747541162426 & Cost test [[0.49542904]]\n",
      "Cost [[0.48831309]] & Log loss 7.329094216860481 & Cost test [[0.49530688]]\n",
      "Cost [[0.48818121]] & Log loss 7.330722316400546 & Cost test [[0.49518548]]\n",
      "Cost [[0.48805016]] & Log loss 7.330708374944782 & Cost test [[0.49506482]]\n",
      "Cost [[0.48791994]] & Log loss 7.332327180181004 & Cost test [[0.49494491]]\n",
      "Cost [[0.48779054]] & Log loss 7.322502817117562 & Cost test [[0.49482574]]\n",
      "Cost [[0.48766196]] & Log loss 7.32740105719352 & Cost test [[0.49470729]]\n",
      "Cost [[0.48753418]] & Log loss 7.324126269505706 & Cost test [[0.49458956]]\n",
      "Cost [[0.4874072]] & Log loss 7.320851481817893 & Cost test [[0.49447254]]\n",
      "Cost [[0.487281]] & Log loss 7.324107680898021 & Cost test [[0.49435623]]\n",
      "Cost [[0.48715559]] & Log loss 7.325735780438085 & Cost test [[0.49424061]]\n",
      "Cost [[0.48703095]] & Log loss 7.320818951754444 & Cost test [[0.49412568]]\n",
      "Cost [[0.48690707]] & Log loss 7.320814304602522 & Cost test [[0.49401143]]\n",
      "Cost [[0.48678395]] & Log loss 7.310994588691003 & Cost test [[0.49389786]]\n",
      "Cost [[0.48666158]] & Log loss 7.314260082074974 & Cost test [[0.49378496]]\n",
      "Cost [[0.48653995]] & Log loss 7.315892828766961 & Cost test [[0.49367271]]\n",
      "Cost [[0.48641906]] & Log loss 7.314246140619209 & Cost test [[0.49356112]]\n",
      "Cost [[0.48629889]] & Log loss 7.314246140619209 & Cost test [[0.49345018]]\n",
      "Cost [[0.48617944]] & Log loss 7.310971352931396 & Cost test [[0.49333988]]\n",
      "Cost [[0.48606071]] & Log loss 7.310971352931396 & Cost test [[0.49323021]]\n",
      "Cost [[0.48594268]] & Log loss 7.306059171399676 & Cost test [[0.49312117]]\n",
      "Cost [[0.48582535]] & Log loss 7.304417130403847 & Cost test [[0.49301275]]\n",
      "Cost [[0.48570872]] & Log loss 7.301146989867955 & Cost test [[0.49290495]]\n",
      "Cost [[0.48559277]] & Log loss 7.30277973655994 & Cost test [[0.49279775]]\n",
      "Cost [[0.4854775]] & Log loss 7.2995142431759685 & Cost test [[0.49269116]]\n",
      "Cost [[0.4853629]] & Log loss 7.297881496483984 & Cost test [[0.49258516]]\n",
      "Cost [[0.48524896]] & Log loss 7.297876849332062 & Cost test [[0.49247975]]\n",
      "Cost [[0.48513569]] & Log loss 7.299500301720206 & Cost test [[0.49237493]]\n",
      "Cost [[0.48502306]] & Log loss 7.2978629078763 & Cost test [[0.49227068]]\n",
      "Cost [[0.48491109]] & Log loss 7.294592767340407 & Cost test [[0.49216701]]\n",
      "Cost [[0.48479975]] & Log loss 7.289685232960608 & Cost test [[0.4920639]]\n",
      "Cost [[0.48468905]] & Log loss 7.286410445272793 & Cost test [[0.49196135]]\n",
      "Cost [[0.48457897]] & Log loss 7.283135657584979 & Cost test [[0.49185936]]\n",
      "Cost [[0.48446952]] & Log loss 7.281498263741073 & Cost test [[0.49175792]]\n",
      "Cost [[0.48436068]] & Log loss 7.278232770357101 & Cost test [[0.49165702]]\n",
      "Cost [[0.48425246]] & Log loss 7.276590729361273 & Cost test [[0.49155667]]\n",
      "Cost [[0.48414483]] & Log loss 7.27331594167346 & Cost test [[0.49145684]]\n",
      "Cost [[0.48403781]] & Log loss 7.270036506833724 & Cost test [[0.49135754]]\n",
      "Cost [[0.48393138]] & Log loss 7.271664606373788 & Cost test [[0.49125877]]\n",
      "Cost [[0.48382553]] & Log loss 7.266757071993989 & Cost test [[0.49116051]]\n",
      "Cost [[0.48372027]] & Log loss 7.263482284306176 & Cost test [[0.49106277]]\n",
      "Cost [[0.48361558]] & Log loss 7.266752424842068 & Cost test [[0.49096554]]\n",
      "Cost [[0.48351147]] & Log loss 7.266752424842068 & Cost test [[0.4908688]]\n",
      "Cost [[0.48340792]] & Log loss 7.265119678150082 & Cost test [[0.49077257]]\n",
      "Cost [[0.48330493]] & Log loss 7.261849537614189 & Cost test [[0.49067683]]\n",
      "Cost [[0.48320249]] & Log loss 7.261849537614189 & Cost test [[0.49058157]]\n",
      "Cost [[0.48310061]] & Log loss 7.261849537614189 & Cost test [[0.4904868]]\n",
      "Cost [[0.48299927]] & Log loss 7.261849537614189 & Cost test [[0.49039251]]\n",
      "Cost [[0.48289846]] & Log loss 7.261844890462268 & Cost test [[0.4902987]]\n",
      "Cost [[0.4827982]] & Log loss 7.261844890462268 & Cost test [[0.49020535]]\n",
      "Cost [[0.48269846]] & Log loss 7.258574749926376 & Cost test [[0.49011247]]\n",
      "Cost [[0.48259925]] & Log loss 7.255304609390484 & Cost test [[0.49002005]]\n",
      "Cost [[0.48250056]] & Log loss 7.243861441090821 & Cost test [[0.48992808]]\n",
      "Cost [[0.48240238]] & Log loss 7.243861441090821 & Cost test [[0.48983657]]\n",
      "Cost [[0.48230471]] & Log loss 7.242224047246914 & Cost test [[0.48974551]]\n",
      "Cost [[0.48220755]] & Log loss 7.24059594770685 & Cost test [[0.48965488]]\n",
      "Cost [[0.48211089]] & Log loss 7.235702354782814 & Cost test [[0.4895647]]\n",
      "Cost [[0.48201473]] & Log loss 7.230804114706857 & Cost test [[0.48947496]]\n",
      "Cost [[0.48191906]] & Log loss 7.225910521782821 & Cost test [[0.48938564]]\n",
      "Cost [[0.48182388]] & Log loss 7.224277775090835 & Cost test [[0.48929675]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost [[0.48172918]] & Log loss 7.224282422242756 & Cost test [[0.48920829]]\n",
      "Cost [[0.48163496]] & Log loss 7.222649675550771 & Cost test [[0.48912024]]\n",
      "Cost [[0.48154121]] & Log loss 7.217751435474813 & Cost test [[0.48903261]]\n",
      "Cost [[0.48144793]] & Log loss 7.214490589242764 & Cost test [[0.48894539]]\n",
      "Cost [[0.48135512]] & Log loss 7.216132630238591 & Cost test [[0.48885858]]\n",
      "Cost [[0.48126278]] & Log loss 7.212871784006543 & Cost test [[0.48877217]]\n",
      "Cost [[0.48117089]] & Log loss 7.206331502934757 & Cost test [[0.48868616]]\n",
      "Cost [[0.48107945]] & Log loss 7.204703403394692 & Cost test [[0.48860055]]\n",
      "Cost [[0.48098847]] & Log loss 7.204708050546614 & Cost test [[0.48851533]]\n",
      "Cost [[0.48089793]] & Log loss 7.203075303854628 & Cost test [[0.4884305]]\n",
      "Cost [[0.48080783]] & Log loss 7.201442557162642 & Cost test [[0.48834605]]\n",
      "Cost [[0.48071817]] & Log loss 7.199805163318736 & Cost test [[0.48826199]]\n",
      "Cost [[0.48062895]] & Log loss 7.199809810470656 & Cost test [[0.48817831]]\n",
      "Cost [[0.48054016]] & Log loss 7.1981770637786715 & Cost test [[0.488095]]\n",
      "Cost [[0.48045179]] & Log loss 7.198181710930593 & Cost test [[0.48801206]]\n",
      "Cost [[0.48036384]] & Log loss 7.198186358082513 & Cost test [[0.4879295]]\n",
      "Cost"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # This is out eta\n",
    "\n",
    "Niteration = 1000\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "loglvec=[]\n",
    "yaxis=[]\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = lrf.sigmoid(xtrain@beta)\n",
    "    gradient = lrf.gradient_ols(xtrain,ytrain,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(xtrain@beta,ytrain.T)\n",
    "    cost_test = lrf.cost_log_ols(xtest@beta,ytest.T)\n",
    "    # Log Loss function from sklearn\n",
    "    logloss=log_loss(ytrain, np.round(xtrain@beta), eps=1e-16, normalize=True)\n",
    "    print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_test)\n",
    "    #costvec.append(cost.ravel())\n",
    "    #loglvec.append(logloss)\n",
    "    #yaxis.append(iter+1)\n",
    "    #plt.plot(costvec)\n",
    "    #plt.plot(loglvec)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 1)\n",
      "(7500, 1)\n"
     ]
    }
   ],
   "source": [
    "print(xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "BÃ¥de egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.94666666666667 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation =lrf.sigmoid(X@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==y)/len(activation),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.15 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(y, predicted_classes)\n",
    "\n",
    "print(accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.X_data_full = X_data \n",
    "#self.Y_data_full = Y_data\n",
    "\n",
    "n_inputs = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "\n",
    "#self.epochs = epochs\n",
    "#self.batch_size = batch_size\n",
    "#self.iterations = self.n_inputs // self.batch_size\n",
    "#self.eta = eta\n",
    "#self.lmbd = lmbd\n",
    "\n",
    "\n",
    "\n",
    "n_hidden_neurons=50  # velge andre verdier om vi vil\n",
    "n_categories=2\n",
    "epochs=100\n",
    "batch_size=80\n",
    "eta=0.0001\n",
    "lmbd=0.1\n",
    "\n",
    "\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 50)\n",
      "(50,)\n",
      "(50, 2)\n",
      "(2,)\n",
      "(30000, 91)\n"
     ]
    }
   ],
   "source": [
    "print(hidden_weights.shape)\n",
    "print(hidden_bias.shape)\n",
    "print(output_weights.shape)\n",
    "print(output_bias.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_h = np.matmul(X, hidden_weights) + hidden_bias\n",
    "a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "\n",
    "exp_term = np.exp(z_o)\n",
    "probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 91)\n",
      "(30000, 50)\n",
      "(30000, 50)\n",
      "(30000, 2)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(z_h.shape)\n",
    "print(a_h.shape)\n",
    "print(z_o.shape)\n",
    "print(probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fourbyfour = np.array([\n",
    "                       [1,2,3,4],\n",
    "                       [3,2,1,4],\n",
    "                       [5,4,6,7],\n",
    "                      ])\n",
    "\n",
    "\n",
    "twobyfourbythree = np.array([\n",
    "                             [[2,3],[11,9],[32,21],[28,17]],\n",
    "                             [[2,3],[1,9],[3,21],[28,7]],\n",
    "                             [[2,3],[1,9],[3,21],[28,7]],\n",
    "                            ])\n",
    "\n",
    "print(np.dot(fourbyfour,fourbyfour.T).shape)\n",
    "print(np.matmul(fourbyfour,twobyfourbythree).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
