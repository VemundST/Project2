{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(prediction):\n",
    "    '''\n",
    "    Sigmoid activation function\n",
    "    '''\n",
    "    return 1. / (1. + np.exp(-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_(prediction):\n",
    "    '''\n",
    "    Relu activation function\n",
    "    '''\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mse_ols(design, data, beta):\n",
    "    '''\n",
    "    Mean squared error\n",
    "    '''\n",
    "    return (data - design.dot(beta)).T*(data - design.dot(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_ols(design, data, beta):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) #logistic regression slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_log_ols(design, data, beta):\n",
    "    '''\n",
    "    Logisitic regression cost function\n",
    "    '''\n",
    "    return -data.dot(np.log(prediction)+1e-10) - ((1-data).dot(np.log(1-prediction + 1e-10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_log_ols(design, data, p):\n",
    "    '''\n",
    "    Gradient w.r.t log\n",
    "    '''\n",
    "    return (1/len(data))*design.T.dot(data-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mse_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Mean squared error\n",
    "    '''\n",
    "    return (data - design.dot(beta)).T*(data - design.dot(beta)) + _lambda(np.sum(beta)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    regu_term = _lambda*np.sum(beta**2) \n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) + _lambda*np.sum(beta**2) + regu_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_log_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Logisitic regression cost function\n",
    "    '''\n",
    "    regu_term = _lambda*np.sum(beta**2) \n",
    "    return -data.dot(np.log(prediction)+1e-10) - ((1-data).dot(np.log(1-prediction + 1e-10))) + regu_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_log_rid(design, data, p, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Gradient w.r.t log\n",
    "    '''\n",
    "    return (1/len(data))*design.T.dot(data-p) +2*_lambda*beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_solver(N, eta, design, data, beta=None):\n",
    "    M=len(data)\n",
    "    if beta != None:\n",
    "        beta = beta\n",
    "    else:\n",
    "        beta = np.random.randn(design.shape[1])\n",
    "     \n",
    "    for i in range(N):\n",
    "        gradients = cost_grad_ols(design,frank,beta)\n",
    "        beta -= eta*gradients\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_x         = 50\n",
    "x           = np.linspace(0, 1, n_x)\n",
    "y           = np.linspace(0, 1, n_x)\n",
    "\n",
    "x_mesh, y_mesh  = np.meshgrid(x,y)\n",
    "noise_level     = 0.01\n",
    "frank           = fx.FrankeFunction(x_mesh, y_mesh, noise_level)\n",
    "\n",
    "frank = np.ravel(frank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = fx.DesignDesign(x,y,10)\n",
    "data = frank.reshape([n_x*n_x,1])\n",
    "np.random.seed(2018)\n",
    "M=len(data)\n",
    "N=10000\n",
    "eta=0.1\n",
    "\n",
    "beta = gradient_solver(N, eta, design, data)\n",
    "\n",
    "\n",
    "prediction = design @ beta\n",
    "pred = np.reshape(prediction,[n_x,n_x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "# Trying to set the seed\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Reading file into data frame\n",
    "directory = os.getcwd()\n",
    "filename = directory + '/cred_card.xls'\n",
    "nanDict = {} # fjerner NaN \n",
    "dataframe = pd.read_excel(filename, header=1, skiprows=0, index_col=0, na_values=nanDict)\n",
    "\n",
    "\n",
    "dataframe.rename(index=str, columns={\"default payment next month\": \"defaultPaymentNextMonth\"}, inplace=True)\n",
    "\n",
    "# Features and targets \n",
    "X = dataframe.loc[:, dataframe.columns != 'defaultPaymentNextMonth'].values\n",
    "y = dataframe.loc[:, dataframe.columns == 'defaultPaymentNextMonth'].values\n",
    "\n",
    "# Categorical variables to one-hot's\n",
    "onehotencoder = OneHotEncoder(categories=\"auto\")\n",
    "\n",
    "X = ColumnTransformer(\n",
    "    [(\"\", onehotencoder, [3]),],\n",
    "    remainder=\"passthrough\"\n",
    ").fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "trainingShare = 0.5 \n",
    "seed  = 1\n",
    "XTrain, XTest, yTrain, yTest=train_test_split(X, y, train_size=trainingShare, \\\n",
    "                                              test_size = 1-trainingShare,\n",
    "                                             random_state=seed)\n",
    "\n",
    "# Input Scaling\n",
    "sc = StandardScaler()\n",
    "XTrain = sc.fit_transform(XTrain)\n",
    "XTest = sc.transform(XTest)\n",
    "\n",
    "# One-hot's of the target vector\n",
    "Y_train_onehot, Y_test_onehot = onehotencoder.fit_transform(yTrain), onehotencoder.fit_transform(yTest)\n",
    "\n",
    "\n",
    "# Remove instances with zeros only for past bill statements or paid amounts\n",
    "'''\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.BILL_AMT1 == 0) &\n",
    "                (dataframe.BILL_AMT2 == 0) &\n",
    "                (dataframe.BILL_AMT3 == 0) &\n",
    "                (dataframe.BILL_AMT4 == 0) &\n",
    "                (dataframe.BILL_AMT5 == 0) &\n",
    "                (dataframe.BILL_AMT6 == 0) &\n",
    "                (dataframe.PAY_AMT1 == 0) &\n",
    "                (dataframe.PAY_AMT2 == 0) &\n",
    "                (dataframe.PAY_AMT3 == 0) &\n",
    "                (dataframe.PAY_AMT4 == 0) &\n",
    "                (dataframe.PAY_AMT5 == 0) &\n",
    "                (dataframe.PAY_AMT6 == 0)].index)\n",
    "'''\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.BILL_AMT1 == 0) &\n",
    "                (dataframe.BILL_AMT2 == 0) &\n",
    "                (dataframe.BILL_AMT3 == 0) &\n",
    "                (dataframe.BILL_AMT4 == 0) &\n",
    "                (dataframe.BILL_AMT5 == 0) &\n",
    "                (dataframe.BILL_AMT6 == 0)].index)\n",
    "\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.PAY_AMT1 == 0) &\n",
    "                (dataframe.PAY_AMT2 == 0) &\n",
    "                (dataframe.PAY_AMT3 == 0) &\n",
    "                (dataframe.PAY_AMT4 == 0) &\n",
    "                (dataframe.PAY_AMT5 == 0) &\n",
    "                (dataframe.PAY_AMT6 == 0)].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#lambdas=np.logspace(-5,7,13)\n",
    "#parameters = [{'C': 1./lambdas, \"solver\":[\"lbfgs\"]}]#*len(parameters)}]\n",
    "#scoring = ['accuracy', 'roc_auc']\n",
    "#logReg = LogisticRegression()\n",
    "#gridSearch = GridSearchCV(logReg, parameters, cv=5, scoring=scoring, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "y=np.squeeze(y)\n",
    "logreg = SGDClassifier(max_iter = 100000, penalty=None, eta0=0.1, learning_rate='constant' )\n",
    "logreg.fit(X,y)\n",
    "prediction = logreg.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.count_nonzero(y-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.79 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (len(y)-count)/len(y)* 100\n",
    "print(accuracy, '%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistic regression.\n",
    "Ulik prøving med dif deffinisjoner.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Activation function used to map any real value between 0 and 1\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def net_input(eta, x):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(x, eta)\n",
    "\n",
    "def probability(eta, x):\n",
    "    '''\n",
    "    Returns the probability after passing through sigmoid\n",
    "    '''\n",
    "    return sigmoid(net_input(eta, x))\n",
    "\n",
    "\n",
    "def cost_grad_ols(design, data, beta):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) #logistic regression slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  from ipykernel import kernelapp as app\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in matmul\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is nan\n",
      "cost is nan\n",
      "cost is nan\n",
      "cost is nan\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is 12581.965949878391\n",
      "cost is 12282.04588155842\n",
      "cost is 12010.683628680812\n",
      "cost is 11764.255185164884\n",
      "cost is 11540.05605997125\n",
      "cost is 11336.05820554089\n",
      "cost is 11150.692810989127\n",
      "cost is 10982.639902069352\n",
      "cost is 10830.643687111677\n",
      "cost is 10693.409294151574\n",
      "cost is 10569.597391258292\n",
      "cost is 10457.848500383821\n",
      "cost is 10356.778349841446\n",
      "cost is 10265.02066402383\n",
      "cost is 10181.50341010486\n",
      "cost is 10105.880160610204\n",
      "cost is 10038.77169687258\n",
      "cost is 9981.509144798349\n",
      "cost is 9935.239476981364\n",
      "cost is 9899.956751541346\n",
      "cost is 9874.257739722463\n",
      "cost is 9855.847431540107\n",
      "cost is 9842.36250665162\n",
      "cost is 9831.973072886545\n",
      "cost is 9823.505965651566\n",
      "cost is 9816.284452461186\n",
      "cost is 9809.931919816236\n",
      "cost is 9804.234605176425\n",
      "cost is 9799.063474286757\n",
      "cost is 9794.333742361212\n",
      "cost is 9789.9846283096\n",
      "cost is 9785.969303585789\n",
      "cost is 9782.249755644021\n",
      "cost is 9778.794075916852\n",
      "cost is 9775.574894166373\n",
      "cost is 9772.568433968921\n",
      "cost is 9769.7538564486\n",
      "cost is 9767.112806792673\n",
      "cost is 9764.629044177409\n",
      "cost is 9762.288166303118\n",
      "cost is 9760.077360946636\n",
      "cost is 9757.985217442494\n",
      "cost is 9756.001545563844\n",
      "cost is 9754.117239665211\n",
      "cost is 9752.32414126334\n",
      "cost is 9750.614939042669\n",
      "cost is 9748.98306213398\n",
      "cost is 9747.422606031927\n",
      "cost is 9745.928248273593\n",
      "cost is 9744.495193691619\n",
      "cost is 9743.119106749005\n",
      "cost is 9741.796071514425\n",
      "cost is 9740.5225365063\n",
      "cost is 9739.295286035169\n",
      "cost is 9738.111394511041\n",
      "cost is 9736.968206633881\n",
      "cost is 9735.86329894877\n",
      "cost is 9734.794466898738\n",
      "cost is 9733.759692117745\n",
      "cost is 9732.757134622378\n",
      "cost is 9731.785105045888\n",
      "cost is 9730.842060196119\n",
      "cost is 9729.926580322368\n",
      "cost is 9729.037365772241\n",
      "cost is 9728.173220708231\n",
      "cost is 9727.333047369662\n",
      "cost is 9726.515839867983\n",
      "cost is 9725.720670044862\n",
      "cost is 9724.94669888468\n",
      "cost is 9724.193143103821\n",
      "cost is 9723.45931881293\n",
      "cost is 9722.744568800557\n",
      "cost is 9722.048366111803\n",
      "cost is 9721.370164802465\n",
      "cost is 9720.709615207094\n",
      "cost is 9720.066268801498\n",
      "cost is 9719.44000170932\n",
      "cost is 9718.830444193167\n",
      "cost is 9718.237792394837\n",
      "cost is 9717.661720362797\n",
      "cost is 9717.102915977077\n",
      "cost is 9716.561022104806\n",
      "cost is 9716.037524861375\n",
      "cost is 9715.531873257773\n",
      "cost is 9715.046896583779\n",
      "cost is 9714.58150406878\n",
      "cost is 9714.14083999636\n",
      "cost is 9713.722555421718\n",
      "cost is 9713.33585222577\n",
      "cost is 9712.975661227563\n",
      "cost is 9712.658384469638\n",
      "cost is 9712.373305076071\n"
     ]
    }
   ],
   "source": [
    "eta = 0.0001 # This is out eta\n",
    "m = 10\n",
    "\n",
    "Niteration = 100\n",
    "beta = np.random.randn(26,1)\n",
    "for iter in range(Niteration):\n",
    "    sigmoid = 1/(1+np.exp(-(XTrain)@(beta))) # vi har ikke definert prediction i vår sigmoid definisjon. \n",
    "    gradients = -(np.transpose(XTrain)@(yTrain-sigmoid))\n",
    "    beta -= eta*gradients\n",
    "  \n",
    "    # Cost function\n",
    "    \n",
    "    #total_cost = -(1 / m) * np.sum(y @ np.log(sigmoid(X))) + (1 - y) @ np.log(1 - sigmoid(X)))\n",
    "    \n",
    "    cost = -np.sum(np.transpose(yTrain)@np.log(sigmoid) + np.transpose(1-yTrain)@np.log(1-sigmoid))\n",
    "    print('cost is', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "Både egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "accuracy() missing 1 required positional argument: 'actual_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-52d14bff31ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mactual_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: accuracy() missing 1 required positional argument: 'actual_classes'"
     ]
    }
   ],
   "source": [
    "def predict(self, x):\n",
    "    theta = parameters[:, np.newaxis]\n",
    "    return probability(theta, x)\n",
    "\n",
    "def accuracy(self, x, actual_classes, probab_threshold=0.5):\n",
    "    predicted_classes = (predict(x) >= \n",
    "                         probab_threshold).astype(int)\n",
    "    predicted_classes = predicted_classes.flatten()\n",
    "    accuracy = np.mean(predicted_classes == actual_classes)\n",
    "    return accuracy * 100\n",
    "accuracy(X, y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.87333333333333 %\n"
     ]
    }
   ],
   "source": [
    "#print(parameters)\n",
    "print(accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
