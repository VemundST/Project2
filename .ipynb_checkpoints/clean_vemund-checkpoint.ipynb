{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(prediction):\n",
    "    '''\n",
    "    Sigmoid activation function, from logistic regression slides. \n",
    "    '''\n",
    "    return 1. / (1. + np.exp(-prediction))\n",
    "    #return np.exp(prediction) / (1. + np.exp(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_(prediction):\n",
    "    '''\n",
    "    Relu activation function\n",
    "    '''\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mse_ols(design, data, beta):\n",
    "    '''\n",
    "    Mean squared error\n",
    "    '''\n",
    "    return (data - design.dot(beta)).T*(data - design.dot(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_ols(design, data, beta):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) #logistic regression slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_log_ols(prediction, data):\n",
    "    '''\n",
    "    Logisitic regression cost function\n",
    "    '''\n",
    "    calc = -data.dot(np.log(sigmoid(prediction)+ 1e-16)) - ((1 - data).dot(np.log(1 - sigmoid(prediction) + 1e-16)))\n",
    "    norm = calc/data.shape[1]\n",
    "    return norm\n",
    "#return -np.mean(data.dot(prediction.T)-np.log(1+np.exp(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_log_ols(design, data, p):\n",
    "    '''\n",
    "    Gradient w.r.t log\n",
    "    '''\n",
    "    return (1/len(data))*design.T.dot(data-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_solver(N, eta, design, data, beta=None):\n",
    "    M=len(data)\n",
    "    if beta != None:\n",
    "        beta = beta\n",
    "    else:\n",
    "        beta = np.random.randn(design.shape[1])\n",
    "     \n",
    "    for i in range(N):\n",
    "        gradients = cost_grad_ols(design,frank,beta)\n",
    "        beta -= eta*gradients\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x         = 50\n",
    "x           = np.linspace(0, 1, n_x)\n",
    "y           = np.linspace(0, 1, n_x)\n",
    "\n",
    "x_mesh, y_mesh  = np.meshgrid(x,y)\n",
    "noise_level     = 0.01\n",
    "frank           = fx.FrankeFunction(x_mesh, y_mesh, noise_level)\n",
    "\n",
    "frank = np.ravel(frank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = fx.DesignDesign(x,y,10)\n",
    "data = frank.reshape([n_x*n_x,1])\n",
    "np.random.seed(2018)\n",
    "M=len(data)\n",
    "N=10000\n",
    "eta=0.1\n",
    "\n",
    "beta = gradient_solver(N, eta, design, data)\n",
    "\n",
    "\n",
    "prediction = design @ beta\n",
    "pred = np.reshape(prediction,[n_x,n_x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[  10000.   16000.   20000.   30000.   40000.   50000.   60000.   70000.\n",
      "   80000.   90000.  100000.  110000.  120000.  130000.  140000.  150000.\n",
      "  160000.  170000.  180000.  190000.  200000.  210000.  220000.  230000.\n",
      "  240000.  250000.  260000.  270000.  280000.  290000.  300000.  310000.\n",
      "  320000.  327680.  330000.  340000.  350000.  360000.  370000.  380000.\n",
      "  390000.  400000.  410000.  420000.  430000.  440000.  450000.  460000.\n",
      "  470000.  480000.  490000.  500000.  510000.  520000.  530000.  540000.\n",
      "  550000.  560000.  570000.  580000.  590000.  600000.  610000.  620000.\n",
      "  630000.  640000.  650000.  660000.  670000.  680000.  690000.  700000.\n",
      "  710000.  720000.  730000.  740000.  750000.  760000.  780000.  800000.\n",
      " 1000000.]\n",
      "[21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38.\n",
      " 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56.\n",
      " 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74.\n",
      " 75. 79.]\n",
      "[-165580. -154973.  -15308. ...  653062.  746814.  964511.]\n",
      "[-69777. -67526. -33350. ... 671563. 743970. 983931.]\n",
      "[-157264.  -61506.  -46127. ...  693131.  855086. 1664089.]\n",
      "[-170000.  -81334.  -65167. ...  628699.  706864.  891586.]\n",
      "[-81334. -61372. -53007. ... 587067. 823540. 927171.]\n",
      "[-339603. -209051. -150953. ...  568638.  699944.  961664.]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 4.93358e+05 5.05000e+05\n",
      " 8.73552e+05]\n",
      "[0.000000e+00 1.000000e+00 2.000000e+00 ... 1.215471e+06 1.227082e+06\n",
      " 1.684259e+06]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 5.08229e+05 8.89043e+05\n",
      " 8.96040e+05]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 4.97000e+05 5.28897e+05\n",
      " 6.21000e+05]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 3.88071e+05 4.17990e+05\n",
      " 4.26529e+05]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 4.43001e+05 5.27143e+05\n",
      " 5.28666e+05]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Trying to set the seed\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Reading file into data frame\n",
    "directory = os.getcwd()\n",
    "filename = directory + '/cred_card.xls'\n",
    "nanDict = {} # fjerner NaN \n",
    "dataframe = pd.read_excel(filename, header=1, skiprows=0, index_col=0, na_values=nanDict)\n",
    "\n",
    "\n",
    "dataframe.rename(index=str, columns={\"default payment next month\": \"defaultPaymentNextMonth\"}, inplace=True)\n",
    "\n",
    "# Features and targets \n",
    "X = dataframe.loc[:, dataframe.columns != 'defaultPaymentNextMonth'].values\n",
    "y = dataframe.loc[:, dataframe.columns == 'defaultPaymentNextMonth'].values\n",
    "\n",
    "\n",
    "# Categorical variables to one-hot's\n",
    "onehotencoder = OneHotEncoder(categories=\"auto\", sparse = False)\n",
    "#print(X.shape)\n",
    "#print(X)\n",
    "\n",
    "X = ColumnTransformer(\n",
    "    [(\"\", onehotencoder, [1,2,3,5,6,7,8,9,10]),],\n",
    "    remainder=\"passthrough\"\n",
    ").fit_transform(X)\n",
    "\n",
    "for i in range(91):\n",
    "    print(np.unique(X[:,i]))\n",
    "\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "trainingShare = 0.5 \n",
    "seed  = 1\n",
    "XTrain, XTest, yTrain, yTest=train_test_split(X, y, train_size=trainingShare, \\\n",
    "                                              test_size = 1-trainingShare,\n",
    "                                             random_state=seed)\n",
    "\n",
    "# Input Scaling\n",
    "#sc = StandardScaler()\n",
    "#XTrain = sc.fit_transform(XTrain)\n",
    "#XTest = sc.transform(XTest)\n",
    "\n",
    "# One-hot's of the target vector\n",
    "Y_train_onehot, Y_test_onehot = onehotencoder.fit_transform(yTrain), onehotencoder.fit_transform(yTest)\n",
    "\n",
    "# Remove instances with zeros only for past bill statements or paid amounts\n",
    "'''\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.BILL_AMT1 == 0) &\n",
    "                (dataframe.BILL_AMT2 == 0) &\n",
    "                (dataframe.BILL_AMT3 == 0) &\n",
    "                (dataframe.BILL_AMT4 == 0) &\n",
    "                (dataframe.BILL_AMT5 == 0) &\n",
    "                (dataframe.BILL_AMT6 == 0) &\n",
    "                (dataframe.PAY_AMT1 == 0) &\n",
    "                (dataframe.PAY_AMT2 == 0) &\n",
    "                (dataframe.PAY_AMT3 == 0) &\n",
    "                (dataframe.PAY_AMT4 == 0) &\n",
    "                (dataframe.PAY_AMT5 == 0) &\n",
    "                (dataframe.PAY_AMT6 == 0)].index)\n",
    "'''\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.BILL_AMT1 == 0) &\n",
    "                (dataframe.BILL_AMT2 == 0) &\n",
    "                (dataframe.BILL_AMT3 == 0) &\n",
    "                (dataframe.BILL_AMT4 == 0) &\n",
    "                (dataframe.BILL_AMT5 == 0) &\n",
    "                (dataframe.BILL_AMT6 == 0)].index)\n",
    "\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.PAY_AMT1 == 0) &\n",
    "                (dataframe.PAY_AMT2 == 0) &\n",
    "                (dataframe.PAY_AMT3 == 0) &\n",
    "                (dataframe.PAY_AMT4 == 0) &\n",
    "                (dataframe.PAY_AMT5 == 0) &\n",
    "                (dataframe.PAY_AMT6 == 0)].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 91)\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0.         0.00606061 0.01010101 0.02020202 0.03030303 0.04040404\n",
      " 0.05050505 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101\n",
      " 0.11111111 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616\n",
      " 0.17171717 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222\n",
      " 0.23232323 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828\n",
      " 0.29292929 0.3030303  0.31313131 0.32088889 0.32323232 0.33333333\n",
      " 0.34343434 0.35353535 0.36363636 0.37373737 0.38383838 0.39393939\n",
      " 0.4040404  0.41414141 0.42424242 0.43434343 0.44444444 0.45454545\n",
      " 0.46464646 0.47474747 0.48484848 0.49494949 0.50505051 0.51515152\n",
      " 0.52525253 0.53535354 0.54545455 0.55555556 0.56565657 0.57575758\n",
      " 0.58585859 0.5959596  0.60606061 0.61616162 0.62626263 0.63636364\n",
      " 0.64646465 0.65656566 0.66666667 0.67676768 0.68686869 0.6969697\n",
      " 0.70707071 0.71717172 0.72727273 0.73737374 0.74747475 0.75757576\n",
      " 0.77777778 0.7979798  1.        ]\n",
      "[0.         0.01724138 0.03448276 0.05172414 0.06896552 0.0862069\n",
      " 0.10344828 0.12068966 0.13793103 0.15517241 0.17241379 0.18965517\n",
      " 0.20689655 0.22413793 0.24137931 0.25862069 0.27586207 0.29310345\n",
      " 0.31034483 0.32758621 0.34482759 0.36206897 0.37931034 0.39655172\n",
      " 0.4137931  0.43103448 0.44827586 0.46551724 0.48275862 0.5\n",
      " 0.51724138 0.53448276 0.55172414 0.56896552 0.5862069  0.60344828\n",
      " 0.62068966 0.63793103 0.65517241 0.67241379 0.68965517 0.70689655\n",
      " 0.72413793 0.74137931 0.75862069 0.77586207 0.79310345 0.81034483\n",
      " 0.82758621 0.84482759 0.86206897 0.87931034 0.89655172 0.9137931\n",
      " 0.93103448 1.        ]\n",
      "[0.         0.00938597 0.13297336 ... 0.72440361 0.8073633  1.        ]\n",
      "[0.         0.00213627 0.0345703  ... 0.70355355 0.77226993 1.        ]\n",
      "[0.         0.0525752  0.06101892 ... 0.4669029  0.55582306 1.        ]\n",
      "[0.         0.0835222  0.0987513  ... 0.75236392 0.82599431 1.        ]\n",
      "[0.         0.01979365 0.02808811 ... 0.66276419 0.89724295 1.        ]\n",
      "[0.         0.10032684 0.14497409 ... 0.69796667 0.79887294 1.        ]\n",
      "[0.00000000e+00 1.14475154e-06 2.28950309e-06 ... 5.64772332e-01\n",
      " 5.78099529e-01 1.00000000e+00]\n",
      "[0.00000000e+00 5.93732912e-07 1.18746582e-06 ... 7.21665136e-01\n",
      " 7.28558969e-01 1.00000000e+00]\n",
      "[0.00000000e+00 1.11602161e-06 2.23204321e-06 ... 5.67194545e-01\n",
      " 9.92191197e-01 1.00000000e+00]\n",
      "[0.00000000e+00 1.61030596e-06 3.22061192e-06 ... 8.00322061e-01\n",
      " 8.51685990e-01 1.00000000e+00]\n",
      "[0.00000000e+00 2.34450647e-06 4.68901294e-06 ... 9.09834970e-01\n",
      " 9.79980259e-01 1.00000000e+00]\n",
      "[0.00000000e+00 1.89155346e-06 3.78310691e-06 ... 8.37960073e-01\n",
      " 9.97119164e-01 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(X.shape)\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X[:,[77,78,79,80,81,82,83,84,85,86,87,88,89,90]])\n",
    "#print(scaler.mean_)\n",
    "#print(scaler.transform(X))\n",
    "scaled = scaler.transform(X[:,[77,78,79,80,81,82,83,84,85,86,87,88,89,90]])\n",
    "X[:,77:91]=scaled\n",
    "for i in range(91):\n",
    "    print(np.unique(X[:,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (loaddata.py, line 64)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3325\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-193-eeebe2314015>\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import loaddata as ld\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\vemundst\\Documents\\PhD\\courses\\ML\\FYS-STK4155\\newprojects\\Project2\\loaddata.py\"\u001b[1;36m, line \u001b[1;32m64\u001b[0m\n\u001b[1;33m    if scaler='standard':\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import loaddata as ld\n",
    "\n",
    "Xt,yt = ld.load_data(scaler='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29966</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29967</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29968</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29969</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29970</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29971</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29972</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29973</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29975</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29976</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29977</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29978</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29979</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29981</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29982</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29983</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29984</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29985</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29988</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29989</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29990</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29991</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29993</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29994</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28497 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SEX  EDUCATION  MARRIAGE  y\n",
       "ID                                \n",
       "1        2          2         1  1\n",
       "2        2          2         2  1\n",
       "3        2          2         2  0\n",
       "4        2          2         1  0\n",
       "5        1          2         1  0\n",
       "6        1          1         2  0\n",
       "7        1          1         2  0\n",
       "8        2          2         2  0\n",
       "9        2          3         1  0\n",
       "10       1          3         2  0\n",
       "11       2          3         2  0\n",
       "12       2          1         2  0\n",
       "13       2          2         2  0\n",
       "14       1          2         2  1\n",
       "15       1          1         2  0\n",
       "16       2          3         3  0\n",
       "17       1          1         2  1\n",
       "18       1          1         1  0\n",
       "21       2          3         2  0\n",
       "22       2          2         1  1\n",
       "23       2          2         2  1\n",
       "24       2          1         1  1\n",
       "25       1          1         2  0\n",
       "26       1          3         2  0\n",
       "27       1          1         2  1\n",
       "28       2          3         2  0\n",
       "29       2          3         1  0\n",
       "30       1          1         2  0\n",
       "31       2          1         2  0\n",
       "32       1          2         2  1\n",
       "...    ...        ...       ... ..\n",
       "29966    1          2         1  0\n",
       "29967    1          5         2  1\n",
       "29968    1          2         1  0\n",
       "29969    1          2         2  0\n",
       "29970    1          2         1  0\n",
       "29971    1          1         1  0\n",
       "29972    1          3         1  0\n",
       "29973    1          1         1  0\n",
       "29975    1          2         1  1\n",
       "29976    1          2         1  0\n",
       "29977    1          2         2  1\n",
       "29978    1          1         2  0\n",
       "29979    1          2         1  0\n",
       "29981    1          3         2  0\n",
       "29982    1          2         1  0\n",
       "29983    1          2         1  1\n",
       "29984    1          2         1  0\n",
       "29985    1          2         2  0\n",
       "29988    1          1         2  0\n",
       "29989    1          1         1  0\n",
       "29990    1          1         2  0\n",
       "29991    1          2         1  0\n",
       "29993    1          3         1  0\n",
       "29994    1          1         2  0\n",
       "29995    1          2         2  1\n",
       "29996    1          3         1  0\n",
       "29997    1          3         2  0\n",
       "29998    1          2         2  1\n",
       "29999    1          3         1  1\n",
       "30000    1          2         1  1\n",
       "\n",
       "[28497 rows x 4 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up datasets\n",
    "\n",
    "data_new = dataframe.rename({'defaultPaymentNextMonth':'y'}, axis='columns')\n",
    "\n",
    "payment = data_new['y']\n",
    "\n",
    "# SEX\n",
    "\n",
    "sex = dataframe['SEX']\n",
    "\n",
    "\n",
    "# Education\n",
    "\n",
    "edu = dataframe['EDUCATION']\n",
    "\n",
    "\n",
    "# Marriage\n",
    "\n",
    "mar = dataframe['MARRIAGE']\n",
    "\n",
    "\n",
    "# Make new dataframe with chosen columns\n",
    "\n",
    "merged_data = pd.concat([sex, edu, mar, payment], axis=1)\n",
    "\n",
    "merged_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[  10000.   16000.   20000.   30000.   40000.   50000.   60000.   70000.\n",
      "   80000.   90000.  100000.  110000.  120000.  130000.  140000.  150000.\n",
      "  160000.  170000.  180000.  190000.  200000.  210000.  220000.  230000.\n",
      "  240000.  250000.  260000.  270000.  280000.  290000.  300000.  310000.\n",
      "  320000.  327680.  330000.  340000.  350000.  360000.  370000.  380000.\n",
      "  390000.  400000.  410000.  420000.  430000.  440000.  450000.  460000.\n",
      "  470000.  480000.  490000.  500000.  510000.  520000.  530000.  540000.\n",
      "  550000.  560000.  570000.  580000.  590000.  600000.  610000.  620000.\n",
      "  630000.  640000.  650000.  660000.  670000.  680000.  690000.  700000.\n",
      "  710000.  720000.  730000.  740000.  750000.  760000.  780000.  800000.\n",
      " 1000000.]\n",
      "[1. 2.]\n",
      "[0. 1. 2. 3. 4. 5. 6.]\n",
      "[21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38.\n",
      " 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56.\n",
      " 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74.\n",
      " 75. 79.]\n",
      "[-2. -1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-165580. -154973.  -15308. ...  653062.  746814.  964511.]\n",
      "[-69777. -67526. -33350. ... 671563. 743970. 983931.]\n",
      "[-157264.  -61506.  -46127. ...  693131.  855086. 1664089.]\n",
      "[-170000.  -81334.  -65167. ...  628699.  706864.  891586.]\n",
      "[-81334. -61372. -53007. ... 587067. 823540. 927171.]\n",
      "[-339603. -209051. -150953. ...  568638.  699944.  961664.]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 4.93358e+05 5.05000e+05\n",
      " 8.73552e+05]\n",
      "[0.000000e+00 1.000000e+00 2.000000e+00 ... 1.215471e+06 1.227082e+06\n",
      " 1.684259e+06]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 5.08229e+05 8.89043e+05\n",
      " 8.96040e+05]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 4.97000e+05 5.28897e+05\n",
      " 6.21000e+05]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 3.88071e+05 4.17990e+05\n",
      " 4.26529e+05]\n",
      "[0.00000e+00 1.00000e+00 2.00000e+00 ... 4.43001e+05 5.27143e+05\n",
      " 5.28666e+05]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(X[:,0]))\n",
    "print(np.unique(X[:,1]))\n",
    "print(np.unique(X[:,2]))\n",
    "print(np.unique(X[:,3]))\n",
    "print(np.unique(X[:,4]))\n",
    "print(np.unique(X[:,5]))\n",
    "print(np.unique(X[:,6]))\n",
    "print(np.unique(X[:,7]))\n",
    "print(np.unique(X[:,8]))\n",
    "print(np.unique(X[:,9]))\n",
    "print(np.unique(X[:,10]))\n",
    "print(np.unique(X[:,11]))\n",
    "print(np.unique(X[:,12]))\n",
    "print(np.unique(X[:,13]))\n",
    "print(np.unique(X[:,14]))\n",
    "print(np.unique(X[:,15]))\n",
    "print(np.unique(X[:,16]))\n",
    "print(np.unique(X[:,17]))\n",
    "print(np.unique(X[:,18]))\n",
    "print(np.unique(X[:,19]))\n",
    "print(np.unique(X[:,20]))\n",
    "print(np.unique(X[:,21]))\n",
    "print(np.unique(X[:,22]))\n",
    "print(np.unique(X[:,23]))\n",
    "print(np.unique(X[:,24]))\n",
    "print(np.unique(X[:,25]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0. 1.]\n",
      "[0.         0.00606061 0.01010101 0.02020202 0.03030303 0.04040404\n",
      " 0.05050505 0.06060606 0.07070707 0.08080808 0.09090909 0.1010101\n",
      " 0.11111111 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616\n",
      " 0.17171717 0.18181818 0.19191919 0.2020202  0.21212121 0.22222222\n",
      " 0.23232323 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828\n",
      " 0.29292929 0.3030303  0.31313131 0.32088889 0.32323232 0.33333333\n",
      " 0.34343434 0.35353535 0.36363636 0.37373737 0.38383838 0.39393939\n",
      " 0.4040404  0.41414141 0.42424242 0.43434343 0.44444444 0.45454545\n",
      " 0.46464646 0.47474747 0.48484848 0.49494949 0.50505051 0.51515152\n",
      " 0.52525253 0.53535354 0.54545455 0.55555556 0.56565657 0.57575758\n",
      " 0.58585859 0.5959596  0.60606061 0.61616162 0.62626263 0.63636364\n",
      " 0.64646465 0.65656566 0.66666667 0.67676768 0.68686869 0.6969697\n",
      " 0.70707071 0.71717172 0.72727273 0.73737374 0.74747475 0.75757576\n",
      " 0.77777778 0.7979798  1.        ]\n",
      "[1. 2.]\n",
      "[0. 1. 2. 3. 4. 5. 6.]\n",
      "[0.         0.01724138 0.03448276 0.05172414 0.06896552 0.0862069\n",
      " 0.10344828 0.12068966 0.13793103 0.15517241 0.17241379 0.18965517\n",
      " 0.20689655 0.22413793 0.24137931 0.25862069 0.27586207 0.29310345\n",
      " 0.31034483 0.32758621 0.34482759 0.36206897 0.37931034 0.39655172\n",
      " 0.4137931  0.43103448 0.44827586 0.46551724 0.48275862 0.5\n",
      " 0.51724138 0.53448276 0.55172414 0.56896552 0.5862069  0.60344828\n",
      " 0.62068966 0.63793103 0.65517241 0.67241379 0.68965517 0.70689655\n",
      " 0.72413793 0.74137931 0.75862069 0.77586207 0.79310345 0.81034483\n",
      " 0.82758621 0.84482759 0.86206897 0.87931034 0.89655172 0.9137931\n",
      " 0.93103448 1.        ]\n",
      "[-2. -1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[-2. -1.  0.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[0.         0.00938597 0.13297336 ... 0.72440361 0.8073633  1.        ]\n",
      "[0.         0.00213627 0.0345703  ... 0.70355355 0.77226993 1.        ]\n",
      "[0.         0.0525752  0.06101892 ... 0.4669029  0.55582306 1.        ]\n",
      "[0.         0.0835222  0.0987513  ... 0.75236392 0.82599431 1.        ]\n",
      "[0.         0.01979365 0.02808811 ... 0.66276419 0.89724295 1.        ]\n",
      "[0.         0.10032684 0.14497409 ... 0.69796667 0.79887294 1.        ]\n",
      "[0.00000000e+00 1.14475154e-06 2.28950309e-06 ... 5.64772332e-01\n",
      " 5.78099529e-01 1.00000000e+00]\n",
      "[0.00000000e+00 5.93732912e-07 1.18746582e-06 ... 7.21665136e-01\n",
      " 7.28558969e-01 1.00000000e+00]\n",
      "[0.00000000e+00 1.11602161e-06 2.23204321e-06 ... 5.67194545e-01\n",
      " 9.92191197e-01 1.00000000e+00]\n",
      "[0.00000000e+00 1.61030596e-06 3.22061192e-06 ... 8.00322061e-01\n",
      " 8.51685990e-01 1.00000000e+00]\n",
      "[0.00000000e+00 2.34450647e-06 4.68901294e-06 ... 9.09834970e-01\n",
      " 9.79980259e-01 1.00000000e+00]\n",
      "[0.00000000e+00 1.89155346e-06 3.78310691e-06 ... 8.37960073e-01\n",
      " 9.97119164e-01 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X[:,[4,7,14,15,16,17,18,19,20,21,22,23,24,25]])\n",
    "#print(scaler.mean_)\n",
    "#print(scaler.transform(X))\n",
    "scaled = scaler.transform(X[:,[4,7,14,15,16,17,18,19,20,21,22,23,24,25]])\n",
    "print(np.unique(X[:,0]))\n",
    "print(np.unique(X[:,1]))\n",
    "print(np.unique(X[:,2]))\n",
    "print(np.unique(X[:,3]))\n",
    "print(np.unique(scaled[:,0]))\n",
    "print(np.unique(X[:,5]))\n",
    "print(np.unique(X[:,6]))\n",
    "print(np.unique(scaled[:,1]))\n",
    "print(np.unique(X[:,8]))\n",
    "print(np.unique(X[:,9]))\n",
    "print(np.unique(X[:,10]))\n",
    "print(np.unique(X[:,11]))\n",
    "print(np.unique(X[:,12]))\n",
    "print(np.unique(X[:,13]))\n",
    "print(np.unique(scaled[:,2]))\n",
    "print(np.unique(scaled[:,3]))\n",
    "print(np.unique(scaled[:,4]))\n",
    "print(np.unique(scaled[:,5]))\n",
    "print(np.unique(scaled[:,6]))\n",
    "print(np.unique(scaled[:,7]))\n",
    "print(np.unique(scaled[:,8]))\n",
    "print(np.unique(scaled[:,9]))\n",
    "print(np.unique(scaled[:,10]))\n",
    "print(np.unique(scaled[:,11]))\n",
    "print(np.unique(scaled[:,12]))\n",
    "print(np.unique(scaled[:,13]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is 169513727.15771538\n",
      "cost is 173592565.08923486\n",
      "cost is 169513736.66230506\n",
      "cost is 173592551.10736522\n",
      "cost is 169513745.3224298\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eta = 0.0001 # This is out eta\n",
    "#m = 10\n",
    "\n",
    "Niteration = 5\n",
    "#beta = np.random.randn(26,1)\n",
    "#\n",
    "beta = parameters.reshape([26,1])\n",
    "\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = sigmoid(XTrain@beta)\n",
    "    gradients = -(np.transpose(XTrain)@(yTrain-sig))\n",
    "    beta -= eta*gradients\n",
    "  \n",
    "    #Cost function\n",
    "    cost = cost_log_ols(XTrain@beta,yTrain)\n",
    " \n",
    "    print('cost is', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is [[1.61336638]]\n",
      "logl is 17.31939576120805\n",
      "cost is [[1.57053352]]\n",
      "logl is 17.177207384447662\n",
      "cost is [[1.52988973]]\n",
      "logl is 17.008127482364685\n",
      "cost is [[1.49140793]]\n",
      "logl is 16.839019697370176\n",
      "cost is [[1.45488033]]\n",
      "logl is 16.694479790762156\n",
      "cost is [[1.42057713]]\n",
      "logl is 16.527807184349864\n",
      "cost is [[1.38828566]]\n",
      "logl is 16.390586754944252\n",
      "cost is [[1.35793141]]\n",
      "logl is 16.23131727446895\n",
      "cost is [[1.32942312]]\n",
      "logl is 16.045177180854708\n",
      "cost is [[1.30261549]]\n",
      "logl is 15.85164093203135\n",
      "cost is [[1.27755597]]\n",
      "logl is 15.584624111340762\n",
      "cost is [[1.2540694]]\n",
      "logl is 15.4082526149669\n",
      "cost is [[1.2320799]]\n",
      "logl is 15.21706789599118\n",
      "cost is [[1.21154033]]\n",
      "logl is 15.045594639693277\n",
      "cost is [[1.19232856]]\n",
      "logl is 14.834844843325255\n",
      "cost is [[1.17440124]]\n",
      "logl is 14.665792824153804\n",
      "cost is [[1.15766474]]\n",
      "logl is 14.506579109501557\n",
      "cost is [[1.14202951]]\n",
      "logl is 14.325253607228685\n",
      "cost is [[1.12743826]]\n",
      "logl is 14.168482041886538\n",
      "cost is [[1.11381904]]\n",
      "logl is 13.992047808961741\n",
      "cost is [[1.10109497]]\n",
      "logl is 13.867052067562375\n",
      "cost is [[1.08921192]]\n",
      "logl is 13.746982449150495\n",
      "cost is [[1.07810073]]\n",
      "logl is 13.612169315415569\n",
      "cost is [[1.06771165]]\n",
      "logl is 13.465061786395578\n",
      "cost is [[1.05798547]]\n",
      "logl is 13.369434573268308\n",
      "cost is [[1.04887306]]\n",
      "logl is 13.295884294122251\n",
      "cost is [[1.04032918]]\n",
      "logl is 13.210095385514188\n",
      "cost is [[1.03230789]]\n",
      "logl is 13.102299250203727\n",
      "cost is [[1.02476723]]\n",
      "logl is 12.99939438424134\n",
      "cost is [[1.01766812]]\n",
      "logl is 12.920945865019329\n",
      "cost is [[1.01097231]]\n",
      "logl is 12.781234491208444\n",
      "cost is [[1.00464679]]\n",
      "logl is 12.683248777505662\n",
      "cost is [[0.99865915]]\n",
      "logl is 12.565642220587522\n",
      "cost is [[0.99298165]]\n",
      "logl is 12.452926933017457\n",
      "cost is [[0.98758854]]\n",
      "logl is 12.399025379998283\n",
      "cost is [[0.98245586]]\n",
      "logl is 12.281418823080145\n",
      "cost is [[0.97756196]]\n",
      "logl is 12.17119447991535\n",
      "cost is [[0.97288812]]\n",
      "logl is 12.1001212037188\n",
      "cost is [[0.96833798]]\n",
      "logl is 12.011897116528521\n",
      "cost is [[0.96405202]]\n",
      "logl is 11.92370091224977\n",
      "cost is [[0.95993762]]\n",
      "logl is 11.857553759040703\n",
      "cost is [[0.95598098]]\n",
      "logl is 11.757091042388415\n",
      "cost is [[0.95217041]]\n",
      "logl is 11.690916006267823\n",
      "cost is [[0.94849503]]\n",
      "logl is 11.61250234068522\n",
      "cost is [[0.94494458]]\n",
      "logl is 11.556095901805016\n",
      "cost is [[0.9415102]]\n",
      "logl is 11.487471745646447\n",
      "cost is [[0.93814882]]\n",
      "logl is 11.423808566114769\n",
      "cost is [[0.93492263]]\n",
      "logl is 11.3674648637855\n",
      "cost is [[0.93171187]]\n",
      "logl is 11.311135102911996\n",
      "cost is [[0.9286665]]\n",
      "logl is 11.264587880734641\n",
      "cost is [[0.92567989]]\n",
      "logl is 11.171500407107814\n",
      "cost is [[0.92279264]]\n",
      "logl is 11.127430187879964\n",
      "cost is [[0.9199771]]\n",
      "logl is 11.098047718152104\n",
      "cost is [[0.91721211]]\n",
      "logl is 11.056391765322823\n",
      "cost is [[0.91452765]]\n",
      "logl is 11.03190056494304\n",
      "cost is [[0.91188959]]\n",
      "logl is 10.995135881461836\n",
      "cost is [[0.9092868]]\n",
      "logl is 10.968202531771956\n",
      "cost is [[0.90676131]]\n",
      "logl is 10.95107263296187\n",
      "cost is [[0.90428839]]\n",
      "logl is 10.904504498600868\n",
      "cost is [[0.90186499]]\n",
      "logl is 10.87267290883503\n",
      "cost is [[0.8994657]]\n",
      "logl is 10.840841319069193\n",
      "cost is [[0.89705586]]\n",
      "logl is 10.818813180183149\n",
      "cost is [[0.89476644]]\n",
      "logl is 10.774729019499537\n",
      "cost is [[0.89250059]]\n",
      "logl is 10.750251760575514\n",
      "cost is [[0.89020767]]\n",
      "logl is 10.725739648012086\n",
      "cost is [[0.8880173]]\n",
      "logl is 10.70369756767028\n",
      "cost is [[0.88587671]]\n",
      "logl is 10.686560698132311\n",
      "cost is [[0.88375965]]\n",
      "logl is 10.67186597790444\n",
      "cost is [[0.88167753]]\n",
      "logl is 10.654736079094357\n",
      "cost is [[0.87959105]]\n",
      "logl is 10.625339667910733\n",
      "cost is [[0.87757184]]\n",
      "logl is 10.591017133739623\n",
      "cost is [[0.87558148]]\n",
      "logl is 10.55916463179014\n",
      "cost is [[0.87358628]]\n",
      "logl is 10.527312129840654\n",
      "cost is [[0.87162697]]\n",
      "logl is 10.50527004949885\n",
      "cost is [[0.86972235]]\n",
      "logl is 10.488112267777236\n",
      "cost is [[0.86783937]]\n",
      "logl is 10.470982368967153\n",
      "cost is [[0.86594374]]\n",
      "logl is 10.444014165637862\n",
      "cost is [[0.86411189]]\n",
      "logl is 10.431775536175849\n",
      "cost is [[0.86228916]]\n",
      "logl is 10.417073845220097\n",
      "cost is [[0.8604761]]\n",
      "logl is 10.399909092770603\n",
      "cost is [[0.85870525]]\n",
      "logl is 10.382765252504754\n",
      "cost is [[0.85695346]]\n",
      "logl is 10.365614441511024\n",
      "cost is [[0.85521925]]\n",
      "logl is 10.336204088871636\n",
      "cost is [[0.85350357]]\n",
      "logl is 10.311705917763968\n",
      "cost is [[0.85181157]]\n",
      "logl is 10.28965686669428\n",
      "cost is [[0.85013707]]\n",
      "logl is 10.270042994206808\n",
      "cost is [[0.84848758]]\n",
      "logl is 10.265137783402968\n",
      "cost is [[0.84685538]]\n",
      "logl is 10.255334332523173\n",
      "cost is [[0.84524316]]\n",
      "logl is 10.240632641567421\n",
      "cost is [[0.84364869]]\n",
      "logl is 10.208787110345819\n",
      "cost is [[0.84206836]]\n",
      "logl is 10.19164327007997\n",
      "cost is [[0.84050827]]\n",
      "logl is 10.176934608396335\n",
      "cost is [[0.83896552]]\n",
      "logl is 10.157334677364627\n",
      "cost is [[0.83744144]]\n",
      "logl is 10.145089077174733\n",
      "cost is [[0.83593101]]\n",
      "logl is 10.125503087598787\n",
      "cost is [[0.8344396]]\n",
      "logl is 10.113271428864659\n",
      "cost is [[0.8329646]]\n",
      "logl is 10.093678468560832\n",
      "cost is [[0.83150376]]\n",
      "logl is 10.069173326725283\n",
      "cost is [[0.83005961]]\n",
      "logl is 10.054478606497412\n",
      "cost is [[0.8286314]]\n",
      "logl is 10.039776915541658\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # This is out eta\n",
    "\n",
    "Niteration = 100\n",
    "beta = np.random.randn(26,1)\n",
    "#\n",
    "#beta = parameters.reshape([26,1])\n",
    "\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = sigmoid(XTrain@beta)\n",
    "    #gradients = -(np.transpose(XTrain)@(yTrain-sig))\n",
    "    gradient = np.dot(XTrain.T, (sig - yTrain)) / yTrain.shape[0]\n",
    "    beta -= eta*gradient\n",
    "    #beta , pred  = fx.OridinaryLeastSquares(XTrain, yTrain,XTrain)\n",
    "    #Cost function\n",
    "    cost = cost_log_ols(XTrain@beta,yTrain.T)\n",
    "    logloss=log_loss(yTrain, np.round(XTrain@beta), eps=1e-16, normalize=True)\n",
    "    #cost = (-yTrain * np.log(sig) - (1 - yTrain) * np.log(1 - sig)).mean()\n",
    "    print('cost is', cost)\n",
    "    print('logl is', logloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.96 %\n"
     ]
    }
   ],
   "source": [
    "#pred_classes = sigmoid()\n",
    "\n",
    "#print(np.array_equal(pred_classes, np.round(pred_classes)))\n",
    "#print(pred_classes)\n",
    "\n",
    "#beta = parameters.reshape([26,1])\n",
    "activation =sigmoid(XTrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "\n",
    "\n",
    "\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==yTrain)/len(activation),'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "Både egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation =sigmoid(X@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "\n",
    "for i in range (len(activation)):\n",
    "    if activation[i]>=0.5:\n",
    "        classes[i] = 1 \n",
    "    else:\n",
    "        classes[1] = 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "print(activation)\n",
    "print(np.array_equal(classes,activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.357454020076612"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(XTrain, yTrain)\n",
    "predicted_classes = model.predict(XTrain)\n",
    "accuracy = accuracy_score(yTrain.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(yTrain, predicted_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.59333333333333 %\n"
     ]
    }
   ],
   "source": [
    "#print(parameters)\n",
    "print(accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips fra gruppelærer: \n",
    "Lage et enklere dataset som har x med 1000 elementer og første 500 verdiene = 0 og de siste etter = 1, slik at y fra 0 - 500 = 0 osv. Sjekker man logisitic regression på dette så vil accuracy være 100% med scikit. Kan bruke cost-funksjon med log for OLS. Ikke nødvenig å kjøre for Ridge og Lasso, dette er tidskrevende for oppgaven. Han tror ikke vi har tid til dette. Var inne på god tanke med loopen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eget dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.zeros([999, 1])\n",
    "y_test = np.zeros([999, 1])\n",
    "\n",
    "y_test[500:999, 0] = 1\n",
    "\n",
    "x_test[500:999, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_test)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_test, y_test)\n",
    "predicted_classes = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 %\n"
     ]
    }
   ],
   "source": [
    "#print(parameters)\n",
    "print(accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.random.randn(26,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.53277921]\n",
      " [ 1.46935877]\n",
      " [ 0.15494743]\n",
      " [ 0.37816252]\n",
      " [-0.88778575]\n",
      " [-1.98079647]\n",
      " [-0.34791215]\n",
      " [ 0.15634897]\n",
      " [ 1.23029068]\n",
      " [ 1.20237985]\n",
      " [-0.38732682]\n",
      " [-0.30230275]\n",
      " [-1.04855297]\n",
      " [-1.42001794]\n",
      " [-1.70627019]\n",
      " [ 1.9507754 ]\n",
      " [-0.50965218]\n",
      " [-0.4380743 ]\n",
      " [-1.25279536]\n",
      " [ 0.77749036]\n",
      " [-1.61389785]\n",
      " [-0.21274028]\n",
      " [-0.89546656]\n",
      " [ 0.3869025 ]\n",
      " [-0.51080514]\n",
      " [-1.18063218]]\n"
     ]
    }
   ],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
