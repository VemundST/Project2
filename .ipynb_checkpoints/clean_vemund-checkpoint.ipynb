{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "\n",
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import log_reg_functions as lrf\n",
    "import loaddata as ld\n",
    "\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ) \n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = ld.load_data(scaler='minmax')\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size=0.25, random_state= 0, shuffle=True, stratify=y)\n",
    "\n",
    "xtrain,xval,ytrain,yval = train_test_split(xtrain,ytrain, test_size=0.25, random_state= 0, shuffle=True, stratify=ytrain)\n",
    "# do this because want our test to be seperate, first splitting into test and train, and then splitting traindata \n",
    "#into training and validation\n",
    "\n",
    "nx_train, ny_train = xtrain.shape\n",
    "nx_test, ny_test = xtest.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1-score is better when splitting data 50/50, this indicates that our algorithm learns more. The accuracy is expected to be lower for test data in this case. The dataset is biased, (read more) and therefore will give higher accuracy on biased datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = np.where(y == 1)\n",
    "indices_zero = np.where(y == 0)\n",
    "datapoints = np.random.choice(indices_zero[0], size=y[indices[0]].shape[0], replace=False)\n",
    "\n",
    "x_new = np.vstack((x[indices[0],:],x[datapoints,:]))\n",
    "y_new = np.vstack((y[indices[0]],y[datapoints]))\n",
    "\n",
    "xtrain = x_new\n",
    "ytrain = y_new\n",
    "\n",
    "nx_train, ny_train = xtrain.shape\n",
    "nx_test, ny_test = xtest.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) \n",
    "Egen logistic regression med gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.249404430389404\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = True\n",
    "doprint = False\n",
    "Niteration = 250\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_val=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = lrf.sigmoid(xtrain@beta)\n",
    "    gradient = lrf.gradient_ols(xtrain,ytrain,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(xtrain@beta,ytrain.T)\n",
    "    cost_val = lrf.cost_log_ols(xval@beta,yval.T) # do this for testdata at the end. \n",
    "    #Log Loss function from sklearn\n",
    "    logloss=log_loss(ytrain, np.round(xtrain@beta), eps=1e-16, normalize=True)\n",
    "    if doprint:\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_val)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_val.append(cost_val.ravel())\n",
    "        loglvec.append(logloss)\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_val, 'r')\n",
    "        #plt.plot(xaxis, loglvec, 'g')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()    \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "   \n",
    "# et relevant plot er iterativt plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4528 1313]\n",
      " [ 790  869]]\n"
     ]
    }
   ],
   "source": [
    "# making confusion matrix to check observed data with model predictions. \n",
    "\n",
    "predictions = xtest@beta\n",
    "sig_val = lrf.sigmoid(predictions)\n",
    "sig_val = np.round(sig_val)\n",
    "\n",
    "cm = confusion_matrix(ytest , sig_val.astype(int))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "BÃ¥de egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.70012055455094 % Training Accuracy\n",
      "0.5965033158212041\n",
      "72.05333333333333 % Validation Accuracy\n",
      "0.44919411352487737\n",
      "71.96 % Test Accuracy\n",
      "0.4524863316844571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "print(f1_score(ytrain, classes))\n",
    "\n",
    "activation =lrf.sigmoid(xval@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==yval)/len(activation),'% Validation Accuracy')\n",
    "print(f1_score(yval, classes))\n",
    "\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n",
    "print(f1_score(ytest, classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistisk regresjon med stokastisk gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.221460819244385\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = True\n",
    "doprint = False\n",
    "Niteration = 250\n",
    "batch_size = 3000\n",
    "batch_size_held_out = nx_train-batch_size\n",
    "\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_test=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "indexes = np.arange(nx_train)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    datapoints = np.random.choice(indexes, size=batch_size, replace=False)\n",
    "    batch_x = xtrain[datapoints,:]\n",
    "    batch_y = ytrain[datapoints]\n",
    "    \n",
    "    batch_x_held_out = np.delete(xtrain, datapoints, axis=0)\n",
    "    batch_y_held_out = np.delete(ytrain, datapoints).reshape([batch_size_held_out,1])\n",
    "    \n",
    "    sig = lrf.sigmoid(batch_x@beta)\n",
    "    gradient = lrf.gradient_ols(batch_x,batch_y,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(batch_x@beta,batch_y.T)\n",
    "    cost_test = lrf.cost_log_ols(batch_x_held_out@beta,batch_y_held_out.T)\n",
    "    #Log Loss function from sklearn\n",
    "    if doprint:\n",
    "        logloss=log_loss(batch_y, np.round(batch_x@beta), eps=1e-16, normalize=True)\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_test)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_test.append(cost_test.ravel())\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_test, 'r')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4382 1459]\n",
      " [ 732  927]]\n"
     ]
    }
   ],
   "source": [
    "# making confusion matrix to check observed data with model predictions. \n",
    "\n",
    "predictions = xtest@beta\n",
    "sig_val = lrf.sigmoid(predictions)\n",
    "sig_val = np.round(sig_val)\n",
    "\n",
    "cm = confusion_matrix(ytest , sig_val.astype(int))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.47703703703704 % Training Accuracy\n",
      "0.4567066521264994\n",
      "71.43111111111111 % Validation Accuracy\n",
      "0.4634390651085141\n",
      "70.78666666666666 % Test Accuracy\n",
      "0.45834363411619283\n"
     ]
    }
   ],
   "source": [
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "print(f1_score(ytrain, classes))\n",
    "\n",
    "activation =lrf.sigmoid(xval@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==yval)/len(activation),'% Validation Accuracy')\n",
    "print(f1_score(yval, classes))\n",
    "\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n",
    "print(f1_score(ytest, classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.26280892103676 % Training Accuracy\n",
      "77.36 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "predicted_classes = model.predict(xtrain)\n",
    "accuracy = accuracy_score(ytrain.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytrain, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Training Accuracy')\n",
    "\n",
    "predicted_classes = model.predict(xtest)\n",
    "accuracy = accuracy_score(ytest.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytest, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "onehotencoder = OneHotEncoder(categories=\"auto\", sparse = False)\n",
    "X = ColumnTransformer(\n",
    "    [(\"\", onehotencoder, [1,2,3,5,6,7,8,9,10]),],\n",
    "    remainder=\"passthrough\"\n",
    ").fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) \n",
    "Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.X_data_full = X_data \n",
    "#self.Y_data_full = Y_data\n",
    "\n",
    "n_inputs, n_features = x.shape\n",
    "\n",
    "#self.epochs = epochs\n",
    "#self.batch_size = batch_size\n",
    "#self.iterations = self.n_inputs // self.batch_size\n",
    "eta = 0.001\n",
    "lmbd = 10e-05\n",
    "\n",
    "\n",
    "n_hidden_neurons=50  # velge andre verdier om vi vil\n",
    "n_categories=2\n",
    "epochs=100\n",
    "batch_size=80\n",
    "eta=0.01\n",
    "n_hidden_neurons2=20  # velge andre verdier om vi vil\n",
    "\n",
    "\n",
    "\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_weights2 = np.random.randn(n_hidden_neurons, n_hidden_neurons2)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.1\n",
    "hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.1\n",
    "\n",
    "output_weights = np.random.randn(n_hidden_neurons2, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 50)\n",
      "(50,)\n",
      "(50, 20)\n",
      "(20,)\n",
      "(20, 2)\n",
      "(2,)\n",
      "(30000, 91)\n"
     ]
    }
   ],
   "source": [
    "print(hidden_weights.shape)\n",
    "print(hidden_bias.shape)\n",
    "print(hidden_weights2.shape)\n",
    "print(hidden_bias2.shape)\n",
    "print(output_weights.shape)\n",
    "print(output_bias.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_h = np.matmul(x, hidden_weights) + hidden_bias\n",
    "a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "z_h2 = np.matmul(a_h, hidden_weights2) + hidden_bias2\n",
    "a_h2 = lrf.sigmoid(z_h2)\n",
    "\n",
    "z_o = np.matmul(a_h2, output_weights) + output_bias\n",
    "\n",
    "exp_term = np.exp(z_o)\n",
    "probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 91)\n",
      "(30000, 50)\n",
      "(30000, 50)\n",
      "(30000, 20)\n",
      "(30000, 20)\n",
      "(30000, 2)\n",
      "[[0.86973101 0.13026899]\n",
      " [0.18130462 0.81869538]\n",
      " [0.16498562 0.83501438]\n",
      " ...\n",
      " [0.94564342 0.05435658]\n",
      " [0.03933096 0.96066904]\n",
      " [0.38076984 0.61923016]]\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(z_h.shape)\n",
    "print(a_h.shape)\n",
    "print(z_h2.shape)\n",
    "print(a_h2.shape)\n",
    "print(z_o.shape)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "error_output = probabilities - y\n",
    "error_hidden2 = np.matmul(error_output, output_weights.T) * a_h2 * (1 - a_h2)\n",
    "error_hidden = np.matmul(error_hidden2, hidden_weights2.T) * a_h * (1 - a_h) #a_h * (1 - a_h) is specific for sigmoid\n",
    "\n",
    "\n",
    "output_weights_gradient = np.matmul(a_h2.T, error_output)\n",
    "output_bias_gradient = np.sum(error_output, axis=0)\n",
    "\n",
    "hidden_weights_gradient2 = np.matmul(a_h.T, error_hidden2)\n",
    "hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)\n",
    "\n",
    "hidden_weights_gradient = np.matmul(x.T, error_hidden)\n",
    "hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "if lmbd > 0.0:\n",
    "    output_weights_gradient += lmbd * output_weights\n",
    "    hidden_weights_gradient2 += lmbd * hidden_weights2\n",
    "    hidden_weights_gradient += lmbd * hidden_weights\n",
    "    \n",
    "output_weights -= eta * output_weights_gradient\n",
    "output_bias -= eta * output_bias_gradient\n",
    "\n",
    "hidden_weights2 -= eta * hidden_weights_gradient2\n",
    "hidden_bias2 -= eta * hidden_bias_gradient2\n",
    "\n",
    "hidden_weights -= eta * hidden_weights_gradient\n",
    "hidden_bias -= eta * hidden_bias_gradient\n",
    "\n",
    "\n",
    "z_h = np.matmul(x, hidden_weights) + hidden_bias\n",
    "a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "z_h2 = np.matmul(a_h, hidden_weights2) + hidden_bias2\n",
    "a_h2 = lrf.sigmoid(z_h2)\n",
    "\n",
    "z_o = np.matmul(a_h2, output_weights) + output_bias\n",
    "\n",
    "exp_term = np.exp(z_o)\n",
    "probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x=xtrain\n",
    "y=ytrain\n",
    "\n",
    "onehotencoder = OneHotEncoder(categories=\"auto\", sparse = False)\n",
    "y = ColumnTransformer(\n",
    "    [(\"\", onehotencoder, [0]),],\n",
    "    remainder=\"passthrough\"\n",
    ").fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nan(a):\n",
    "    print(np.isnan(a).sum())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "Niter = 1000\n",
    "\n",
    "\n",
    "n_inputs, n_features = x.shape\n",
    "\n",
    "#self.epochs = epochs\n",
    "#self.batch_size = batch_size\n",
    "#self.iterations = self.n_inputs // self.batch_size\n",
    "#lmbd = 0.1\n",
    "\n",
    "\n",
    "n_hidden_neurons=20  # velge andre verdier om vi vil\n",
    "n_categories=2\n",
    "\n",
    "eta=0.0001\n",
    "n_hidden_neurons2=5  # velge andre verdier om vi vil\n",
    "\n",
    "bias=0\n",
    "\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_weights2 = np.random.randn(n_hidden_neurons, n_hidden_neurons2)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + bias\n",
    "hidden_bias2 = np.zeros(n_hidden_neurons2) + bias\n",
    "\n",
    "output_weights = np.random.randn(n_hidden_neurons2, n_categories)\n",
    "output_bias = np.zeros(n_categories) + bias\n",
    "\n",
    "for i in range(Niter):\n",
    "    z_h = np.matmul(x, hidden_weights) + hidden_bias\n",
    "    #test_nan(z_h)\n",
    "    a_h = lrf.sigmoid(z_h)\n",
    "    #test_nan(a_h)\n",
    "    z_h2 = np.matmul(a_h, hidden_weights2) + hidden_bias2\n",
    "    #test_nan(z_h2)\n",
    "    a_h2 = lrf.sigmoid(z_h2)\n",
    "    #test_nan(a_h2)\n",
    "    z_o = np.matmul(a_h2, output_weights) + output_bias\n",
    "    \n",
    "    #print(np.min(z_o), np.max(z_o))\n",
    "    #probabilities = lrf.sigmoid(z_o)\n",
    "    exp_term = np.exp(z_o)\n",
    "    #test_nan(exp_term)\n",
    "    #print(np.unique(exp_term))\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    #test_nan(probabilities)\n",
    "    \n",
    "    error_output = probabilities - y \n",
    "    #print((1-error_output).sum()/len(error_output))\n",
    "    #print(np.isnan(probabilities).sum())\n",
    "    error_hidden2 = np.matmul(error_output, output_weights.T) * a_h2 * (1 - a_h2)\n",
    "    error_hidden = np.matmul(error_hidden2, hidden_weights2.T) * a_h * (1 - a_h) #a_h * (1 - a_h) is specific for sigmoid\n",
    "\n",
    "    output_weights_gradient = np.matmul(a_h2.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    print(output_weights_gradient.shape)\n",
    "    hidden_weights_gradient2 = np.matmul(a_h.T, error_hidden2)\n",
    "    hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)\n",
    "\n",
    "    hidden_weights_gradient = np.matmul(x.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "    \n",
    "    #if lmbd > 0.0:\n",
    "     #   output_weights_gradient += lmbd * output_weights\n",
    "      #  hidden_weights_gradient2 += lmbd * hidden_weights2\n",
    "       # hidden_weights_gradient += lmbd * hidden_weights\n",
    "    \n",
    "    #test_nan(output_weights_gradient)\n",
    "    #test_nan(output_weights_bias)\n",
    "    \n",
    "    output_weights -= eta * output_weights_gradient\n",
    "    output_bias -= eta * output_bias_gradient\n",
    "    #print(output_weights_gradient)\n",
    "    \n",
    "    \n",
    "    hidden_weights2 -= eta * hidden_weights_gradient2\n",
    "    hidden_bias2 -= eta * hidden_bias_gradient2\n",
    "\n",
    "    hidden_weights -= eta * hidden_weights_gradient\n",
    "    hidden_bias -= eta * hidden_bias_gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def forward_pass():\n",
    "for i in range(Niter):\n",
    "    z_h = np.matmul(xtest, hidden_weights) + hidden_bias\n",
    "    a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "    z_h2 = np.matmul(a_h, hidden_weights2) + hidden_bias2\n",
    "    a_h2 = lrf.sigmoid(z_h2)\n",
    "\n",
    "    z_o = np.matmul(a_h2, output_weights) + output_bias\n",
    "    exp_term = np.exp(z_o)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89081651 0.10918349]\n",
      " [0.91380032 0.08619968]\n",
      " [0.90385102 0.09614898]\n",
      " ...\n",
      " [0.89687279 0.10312721]\n",
      " [0.88505932 0.11494068]\n",
      " [0.93537178 0.06462822]]\n"
     ]
    }
   ],
   "source": [
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "81.8 % Training Accuracy\n",
      "0.4678362573099415\n",
      "[[5535  306]\n",
      " [1059  600]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes=np.argmax(probabilities,axis=1)\n",
    "#y=np.argmax(y,axis=1)\n",
    "print(classes)\n",
    "print(100*np.sum(classes.T==ytest.ravel())/len(ytest.ravel()),'% Training Accuracy')\n",
    "print(f1_score(ytest, classes))\n",
    "\n",
    "cm = confusion_matrix(ytest , classes)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ANN(add_layer):\n",
    "    def __init__(self, lmb=0, bias=0):\n",
    "        self.lmb=lmb\n",
    "        self.bias = bias\n",
    "        self.layers=dict()\n",
    "        self.n_layers=int()\n",
    "        \n",
    "        self.pred=dict()\n",
    "        self.act=dict()\n",
    "        \n",
    "        \n",
    "    def add_layers(self, n_neurons=[20,5], n_features=[20,5], n_layers=2):\n",
    "        self.n_layers=n_layers\n",
    "        for i in range(n_layers):\n",
    "            layer_weights = np.random.randn(n_features[i], n_neurons[i])\n",
    "            self.layers['w'+str(i)] = layer_weights\n",
    "            layer_bias = np.zeros(n_neurons[i]) + self.bias\n",
    "            self.layers['b'+str(i)] = layer_bias\n",
    "    \n",
    "    def feed(self, design, activation=[lrf.sigmoid,lrf.sigmoid, lrf.sigmoid]):\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            \n",
    "            if i==0:\n",
    "                self.pred[str(i)] = np.matmul(design, self.layers['w'+str(i)]) + self.layers['b'+str(i)]\n",
    "                self.act[str(i)] = activation[i](self.pred[str(i)])\n",
    "            else:\n",
    "                self.pred[str(i)] = np.matmul(self.act[str(i-1)], self.layers['w'+str(i)]) + self.layers['b'+str(i)]                \n",
    "                self.act[str(i)] = activation[i](self.pred[str(i)])\n",
    "  \n",
    "    def back(self, design, data):\n",
    "        \n",
    "        for i in np.arange(self.n_layers-1,0,-1):\n",
    "            if i==self.n_layers-1:\n",
    "                error = self.act[str(i)] - data\n",
    "            else:\n",
    "                error = np.matmul(error, self.layers['w'+str(i+1)].T) * self.act[str(i)] * (1 - self.act[str(i)])\n",
    "            if i == 0:\n",
    "                gradients_weights = np.matmul(design.T, error)\n",
    "                gradients_bias = np.sum(error, axis=0)\n",
    "            else:\n",
    "                gradients_weights = np.matmul(self.act[str(i-1)].T, error)\n",
    "                gradients_bias = np.sum(error, axis=0)\n",
    "            \n",
    "            \n",
    "            self.layers['w'+str(i)] -= eta * gradients_weights\n",
    "            self.layers['b'+str(i)] -= eta * gradients_bias\n",
    "   \n",
    "    def feed_out(self, design, activation=[lrf.sigmoid,lrf.sigmoid, lrf.sigmoid]):\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            \n",
    "            if i==0:\n",
    "                self.pred[str(i)] = np.matmul(design, self.layers['w'+str(i)]) + self.layers['b'+str(i)]\n",
    "                self.act[str(i)] = activation[i](self.pred[str(i)])\n",
    "            else:\n",
    "                self.pred[str(i)] = np.matmul(self.act[str(i-1)], self.layers['w'+str(i)]) + self.layers['b'+str(i)]                \n",
    "                self.act[str(i)] = activation[i](self.pred[str(i)])\n",
    "        return self.act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n",
      "(30000, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'rint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'round'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-378-d8e53fe976c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'% Training Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mround_\u001b[1;34m(a, decimals, out)\u001b[0m\n\u001b[0;32m   3380\u001b[0m     \u001b[0maround\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mequivalent\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0msee\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3381\u001b[0m     \"\"\"\n\u001b[1;32m-> 3382\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36maround\u001b[1;34m(a, decimals, out)\u001b[0m\n\u001b[0;32m   3005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3006\u001b[0m     \"\"\"\n\u001b[1;32m-> 3007\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'round'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# a downstream library like 'pandas'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'rint'"
     ]
    }
   ],
   "source": [
    "neural_net = ANN(lmb=0.3, bias=0.24)            \n",
    "neural_net.add_layers(n_features=[91,50,20], n_neurons = [50,20,1] , n_layers=3)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    neural_net.feed(x)\n",
    "    neural_net.back(x,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16962899]\n",
      " [0.17562322]\n",
      " [0.09653523]\n",
      " ...\n",
      " [0.25818784]\n",
      " [0.10053432]\n",
      " [0.09439429]]\n",
      "0.0 % Training Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [16875, 30000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-384-25a960179f77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'% Training Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m   1058\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1059\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1180\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1182\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1183\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1415\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                          str(average_options))\n\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [16875, 30000]"
     ]
    }
   ],
   "source": [
    "activation=neural_net.feed_out(x)['2']\n",
    "print(activation)\n",
    "\n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "print(f1_score(ytrain, classes))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(3-1,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-661f946c938d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mneural_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlmb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mneural_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neurons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneural_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weights1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-209-661f946c938d>\u001b[0m in \u001b[0;36madd_layers\u001b[1;34m(self, n_neurons, n_features, n_layers)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mlayer_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neurons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mlayer_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neurons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "class ANN(add_layer):\n",
    "    def __init__(self, lmb=0, bias=0):\n",
    "        self.lmb=lmb\n",
    "        self.bias = bias\n",
    "        self.layers=[]\n",
    "        self.biases=[]\n",
    "    \n",
    "    def add_layers(self, n_neurons=[20,5], n_features=[20,5], n_layers=2):\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            layer_weights = np.random.randn(n_features[i], n_neurons[i])\n",
    "            self.layers[i] = layer_weights\n",
    "            layer_bias = np.zeros(n_neurons[i]) + self.bias\n",
    "            self.biases[i] = layer_bias\n",
    "\n",
    "neural_net = ANN(lmb=0.3, bias=0.24)            \n",
    "neural_net.add_layers(n_neurons = [3,2], n_features=[2,2], n_layers=2)\n",
    "print(neural_net.layers['weights1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(5-1,-1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function log_reg_functions.sigmoid(prediction)>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation=[lrf.sigmoid,lrf.sigmoid]\n",
    "activation[1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()['bias'+str(1)]=3\n",
    "bias1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
