{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(prediction):\n",
    "    '''\n",
    "    Sigmoid activation function, from logistic regression slides. \n",
    "    '''\n",
    "    return 1. / (1. + np.exp(-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_(prediction):\n",
    "    '''\n",
    "    Relu activation function\n",
    "    '''\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mse_ols(design, data, beta):\n",
    "    '''\n",
    "    Mean squared error\n",
    "    '''\n",
    "    return (data - design.dot(beta)).T*(data - design.dot(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_ols(design, data, beta):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) #logistic regression slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_log_ols(design, data, beta):\n",
    "    '''\n",
    "    Logisitic regression cost function\n",
    "    '''\n",
    "    return -data.dot(np.log(prediction)+1e-10) - ((1-data).dot(np.log(1-prediction + 1e-10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_log_ols(design, data, p):\n",
    "    '''\n",
    "    Gradient w.r.t log\n",
    "    '''\n",
    "    return (1/len(data))*design.T.dot(data-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mse_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Mean squared error\n",
    "    '''\n",
    "    return (data - design.dot(beta)).T*(data - design.dot(beta)) + _lambda(np.sum(beta)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    regu_term = _lambda*np.sum(beta**2) \n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) + _lambda*np.sum(beta**2) + regu_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_log_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Logisitic regression cost function\n",
    "    '''\n",
    "    regu_term = _lambda*np.sum(beta**2) \n",
    "    return -data.dot(np.log(prediction)+1e-10) - ((1-data).dot(np.log(1-prediction + 1e-10))) + regu_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_log_rid(design, data, p, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Gradient w.r.t log\n",
    "    '''\n",
    "    return (1/len(data))*design.T.dot(data-p) +2*_lambda*beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_solver(N, eta, design, data, beta=None):\n",
    "    M=len(data)\n",
    "    if beta != None:\n",
    "        beta = beta\n",
    "    else:\n",
    "        beta = np.random.randn(design.shape[1])\n",
    "     \n",
    "    for i in range(N):\n",
    "        gradients = cost_grad_ols(design,frank,beta)\n",
    "        beta -= eta*gradients\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_x         = 50\n",
    "x           = np.linspace(0, 1, n_x)\n",
    "y           = np.linspace(0, 1, n_x)\n",
    "\n",
    "x_mesh, y_mesh  = np.meshgrid(x,y)\n",
    "noise_level     = 0.01\n",
    "frank           = fx.FrankeFunction(x_mesh, y_mesh, noise_level)\n",
    "\n",
    "frank = np.ravel(frank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = fx.DesignDesign(x,y,10)\n",
    "data = frank.reshape([n_x*n_x,1])\n",
    "np.random.seed(2018)\n",
    "M=len(data)\n",
    "N=10000\n",
    "eta=0.1\n",
    "\n",
    "beta = gradient_solver(N, eta, design, data)\n",
    "\n",
    "\n",
    "prediction = design @ beta\n",
    "pred = np.reshape(prediction,[n_x,n_x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "# Trying to set the seed\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Reading file into data frame\n",
    "directory = os.getcwd()\n",
    "filename = directory + '/cred_card.xls'\n",
    "nanDict = {} # fjerner NaN \n",
    "dataframe = pd.read_excel(filename, header=1, skiprows=0, index_col=0, na_values=nanDict)\n",
    "\n",
    "\n",
    "dataframe.rename(index=str, columns={\"default payment next month\": \"defaultPaymentNextMonth\"}, inplace=True)\n",
    "\n",
    "# Features and targets \n",
    "X = dataframe.loc[:, dataframe.columns != 'defaultPaymentNextMonth'].values\n",
    "y = dataframe.loc[:, dataframe.columns == 'defaultPaymentNextMonth'].values\n",
    "\n",
    "# Categorical variables to one-hot's\n",
    "onehotencoder = OneHotEncoder(categories=\"auto\")\n",
    "\n",
    "X = ColumnTransformer(\n",
    "    [(\"\", onehotencoder, [3]),],\n",
    "    remainder=\"passthrough\"\n",
    ").fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "trainingShare = 0.5 \n",
    "seed  = 1\n",
    "XTrain, XTest, yTrain, yTest=train_test_split(X, y, train_size=trainingShare, \\\n",
    "                                              test_size = 1-trainingShare,\n",
    "                                             random_state=seed)\n",
    "\n",
    "# Input Scaling\n",
    "sc = StandardScaler()\n",
    "XTrain = sc.fit_transform(XTrain)\n",
    "XTest = sc.transform(XTest)\n",
    "\n",
    "# One-hot's of the target vector\n",
    "Y_train_onehot, Y_test_onehot = onehotencoder.fit_transform(yTrain), onehotencoder.fit_transform(yTest)\n",
    "\n",
    "\n",
    "# Remove instances with zeros only for past bill statements or paid amounts\n",
    "'''\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.BILL_AMT1 == 0) &\n",
    "                (dataframe.BILL_AMT2 == 0) &\n",
    "                (dataframe.BILL_AMT3 == 0) &\n",
    "                (dataframe.BILL_AMT4 == 0) &\n",
    "                (dataframe.BILL_AMT5 == 0) &\n",
    "                (dataframe.BILL_AMT6 == 0) &\n",
    "                (dataframe.PAY_AMT1 == 0) &\n",
    "                (dataframe.PAY_AMT2 == 0) &\n",
    "                (dataframe.PAY_AMT3 == 0) &\n",
    "                (dataframe.PAY_AMT4 == 0) &\n",
    "                (dataframe.PAY_AMT5 == 0) &\n",
    "                (dataframe.PAY_AMT6 == 0)].index)\n",
    "'''\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.BILL_AMT1 == 0) &\n",
    "                (dataframe.BILL_AMT2 == 0) &\n",
    "                (dataframe.BILL_AMT3 == 0) &\n",
    "                (dataframe.BILL_AMT4 == 0) &\n",
    "                (dataframe.BILL_AMT5 == 0) &\n",
    "                (dataframe.BILL_AMT6 == 0)].index)\n",
    "\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.PAY_AMT1 == 0) &\n",
    "                (dataframe.PAY_AMT2 == 0) &\n",
    "                (dataframe.PAY_AMT3 == 0) &\n",
    "                (dataframe.PAY_AMT4 == 0) &\n",
    "                (dataframe.PAY_AMT5 == 0) &\n",
    "                (dataframe.PAY_AMT6 == 0)].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistic regression.\n",
    "Ulik prøving med dif deffinisjoner.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Activation function used to map any real value between 0 and 1\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def net_input(eta, x):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(x, eta)\n",
    "\n",
    "def probability(eta, x):\n",
    "    '''\n",
    "    Returns the probability after passing through sigmoid\n",
    "    '''\n",
    "    return sigmoid(net_input(eta, x))\n",
    "\n",
    "\n",
    "def cost_grad_ols(design, data, beta):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) #logistic regression slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is nan\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is 10982.241336502719\n",
      "cost is 10805.358764371216\n",
      "cost is 10652.930129351927\n",
      "cost is 10521.735475393241\n",
      "cost is 10408.88097316435\n",
      "cost is 10312.058125847805\n",
      "cost is 10229.525557031535\n",
      "cost is 10159.880848230472\n",
      "cost is 10101.78494188874\n",
      "cost is 10053.801014999724\n",
      "cost is 10014.407822586523\n",
      "cost is 9982.09697245088\n",
      "cost is 9955.474376315311\n",
      "cost is 9933.332708833306\n",
      "cost is 9914.680595584232\n",
      "cost is 9898.735232007868\n",
      "cost is 9884.895008701818\n",
      "cost is 9872.706404587136\n",
      "cost is 9861.832767758093\n",
      "cost is 9852.027000503032\n",
      "cost is 9843.108299938775\n",
      "cost is 9834.943256471211\n",
      "cost is 9827.431479040497\n",
      "cost is 9820.495316249162\n",
      "cost is 9814.072826264088\n",
      "cost is 9808.113092164389\n",
      "cost is 9802.57313317157\n",
      "cost is 9797.415865620143\n",
      "cost is 9792.608742009385\n",
      "cost is 9788.122825654642\n",
      "cost is 9783.932145056651\n",
      "cost is 9780.013229593287\n",
      "cost is 9776.344763821853\n",
      "cost is 9772.907321104329\n",
      "cost is 9769.683151150155\n",
      "cost is 9766.656005524392\n",
      "cost is 9763.810990457834\n",
      "cost is 9761.13444018704\n",
      "cost is 9758.613806074052\n",
      "cost is 9756.237558377126\n",
      "cost is 9753.995098397716\n",
      "cost is 9751.876679330744\n",
      "cost is 9749.87333467584\n",
      "cost is 9747.976813101546\n",
      "cost is 9746.179519227326\n",
      "cost is 9744.474459394249\n",
      "cost is 9742.855192295552\n",
      "cost is 9741.315783529017\n",
      "cost is 9739.850764280818\n",
      "cost is 9738.45509308484\n",
      "cost is 9737.124121195628\n",
      "cost is 9735.853560323067\n",
      "cost is 9734.639453613632\n",
      "cost is 9733.478148363887\n",
      "cost is 9732.366271731973\n",
      "cost is 9731.300707606173\n",
      "cost is 9730.278576323053\n",
      "cost is 9729.297215001003\n",
      "cost is 9728.354160664418\n",
      "cost is 9727.447133459002\n",
      "cost is 9726.574022682265\n",
      "cost is 9725.732872384666\n",
      "cost is 9724.921869891066\n",
      "cost is 9724.13933336376\n",
      "cost is 9723.38370247119\n",
      "cost is 9722.653527548466\n",
      "cost is 9721.947462130885\n",
      "cost is 9721.264253396934\n",
      "cost is 9720.602736336994\n",
      "cost is 9719.961825204278\n",
      "cost is 9719.340509135096\n",
      "cost is 9718.737844366087\n",
      "cost is 9718.152951162714\n",
      "cost is 9717.585006587538\n",
      "cost is 9717.033242629155\n",
      "cost is 9716.496939326862\n",
      "cost is 9715.975424024176\n",
      "cost is 9715.468064670793\n",
      "cost is 9714.974270153622\n",
      "cost is 9714.49348360816\n",
      "cost is 9714.025183806345\n",
      "cost is 9713.568878317032\n",
      "cost is 9713.124105954104\n",
      "cost is 9712.69042963058\n",
      "cost is 9712.267439892255\n",
      "cost is 9711.854747313348\n",
      "cost is 9711.45198715973\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "eta = 0.0001 # This is out eta\n",
    "#m = 10\n",
    "\n",
    "Niteration = 100\n",
    "beta = np.random.randn(26,1)\n",
    "#\n",
    "#beta = parameters.reshape([26,1])\n",
    "\n",
    "for iter in range(Niteration):\n",
    "    sigmoid = 1/(1+np.exp(-(XTrain)@(beta))) # vi har ikke definert prediction i vår sigmoid definisjon. \n",
    "    gradients = -(np.transpose(XTrain)@(yTrain-sigmoid))\n",
    "    beta -= eta*gradients\n",
    "  \n",
    "    #Cost function\n",
    "    \n",
    "    #total_cost = -(1 / m) * np.sum(y @ np.log(sigmoid(X))) + (1 - y) @ np.log(1 - sigmoid(X))\n",
    "    \n",
    "    cost = -np.sum(np.transpose(yTrain)@np.log(sigmoid) + np.transpose(1-yTrain)@np.log(1-sigmoid))\n",
    "    print('cost is', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "Både egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation =sigmoid(X@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "\n",
    "for i in range (len(activation)):\n",
    "    if activation[i]>=0.5:\n",
    "        classes[i] = 1 \n",
    "    else:\n",
    "        classes[1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "print(activation)\n",
    "print(np.array_equal(classes,activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "model = LogisticRegression(verbose=1)\n",
    "model.fit(X, y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.80507869e-06 -1.22998119e-06 -5.41513255e-04 -6.27505749e-06\n",
      "  -3.38009109e-06 -1.01702502e-03 -1.17028712e-03 -1.43422974e-02\n",
      "   1.96469477e-03  1.53074813e-03  1.34269153e-03  1.24178331e-03\n",
      "   1.14722417e-03  1.06222775e-03 -8.77043218e-06  5.38608128e-06\n",
      "   2.05007022e-06  6.95891486e-07  3.13818421e-06  1.97716051e-06\n",
      "  -3.01411754e-05 -2.05357927e-05 -8.50155700e-06 -9.19856313e-06\n",
      "  -5.84077430e-06 -2.06600438e-06]]\n",
      "77.87333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(parameters)\n",
    "print(accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips fra gruppelærer: \n",
    "Lage et enklere dataset som har x med 1000 elementer og første 500 verdiene = 0 og de siste etter = 1, slik at y fra 0 - 500 = 0 osv. Sjekker man logisitic regression på dette så vil accuracy være 100% med scikit. Kan bruke cost-funksjon med log for OLS. Ikke nødvenig å kjøre for Ridge og Lasso, dette er tidskrevende for oppgaven. Han tror ikke vi har tid til dette. Var inne på god tanke med loopen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eget dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.zeros([999, 1])\n",
    "y_test = np.zeros([999, 1])\n",
    "\n",
    "y_test[500:999, 0] = 1\n",
    "\n",
    "x_test[500:999, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_test)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_test, y_test)\n",
    "predicted_classes = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 %\n"
     ]
    }
   ],
   "source": [
    "#print(parameters)\n",
    "print(accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.random.randn(26,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.53277921]\n",
      " [ 1.46935877]\n",
      " [ 0.15494743]\n",
      " [ 0.37816252]\n",
      " [-0.88778575]\n",
      " [-1.98079647]\n",
      " [-0.34791215]\n",
      " [ 0.15634897]\n",
      " [ 1.23029068]\n",
      " [ 1.20237985]\n",
      " [-0.38732682]\n",
      " [-0.30230275]\n",
      " [-1.04855297]\n",
      " [-1.42001794]\n",
      " [-1.70627019]\n",
      " [ 1.9507754 ]\n",
      " [-0.50965218]\n",
      " [-0.4380743 ]\n",
      " [-1.25279536]\n",
      " [ 0.77749036]\n",
      " [-1.61389785]\n",
      " [-0.21274028]\n",
      " [-0.89546656]\n",
      " [ 0.3869025 ]\n",
      " [-0.51080514]\n",
      " [-1.18063218]]\n"
     ]
    }
   ],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
