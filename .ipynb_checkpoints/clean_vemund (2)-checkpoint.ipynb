{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "\n",
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import log_reg_functions as lrf\n",
    "import loaddata as ld\n",
    "\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ) \n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "x,y = ld.load_data(scaler='minmax')\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size=0.25, random_state= 0, shuffle=True) #, stratify=[0,1])\n",
    "\n",
    "xtrain,xval,ytrain,yval = train_test_split(xtrain,ytrain, test_size=0.25, random_state= 0, shuffle=True) #, stratify=[0,1])\n",
    "# do this because want our test to be seperate, first splitting into test and train, and then splitting traindata \n",
    "#into training and validation\n",
    "\n",
    "nx_train, ny_train = xtrain.shape\n",
    "nx_test, ny_test = xtest.shape\n",
    "\n",
    "print(yval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) \n",
    "Egen logistic regression med gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.530029773712158\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = False\n",
    "doprint = False\n",
    "Niteration = 1000\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_val=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = lrf.sigmoid(xtrain@beta)\n",
    "    gradient = lrf.gradient_ols(xtrain,ytrain,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(xtrain@beta,ytrain.T)\n",
    "    cost_val = lrf.cost_log_ols(xval@beta,yval.T) # do this for testdata at the end. \n",
    "    #Log Loss function from sklearn\n",
    "    if doprint:\n",
    "        logloss=log_loss(ytrain, np.round(xtrain@beta), eps=1e-16, normalize=True)\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_val)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_val.append(cost_val.ravel())\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_val, 'r')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()    \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "   \n",
    "# et relevant plot er iterativt plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5625, 1)\n",
      "[[4412    0]\n",
      " [1213    0]]\n"
     ]
    }
   ],
   "source": [
    "# making confusion matrix to check observed data with model predictions. \n",
    "\n",
    "predictions = xval@beta\n",
    "\n",
    "sig_val = lrf.sigmoid(predictions)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "for i in range (len(sig_val)): \n",
    "    if sig_val[i] >= 0.5:\n",
    "        sig_val[i] == 1\n",
    "    if sig_val[i] < 0.5:\n",
    "        sig_val[i] == 0\n",
    "\n",
    "\n",
    "cm = confusion_matrix(yval , sig_val.astype(int))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "BÃ¥de egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.23851851851852 % Training Accuracy\n",
      "82.01333333333334 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistisk regresjon med stokastisk gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.83239507675171\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = False\n",
    "doprint = False\n",
    "Niteration = 1000\n",
    "batch_size = 3000\n",
    "batch_size_held_out = nx_train-batch_size\n",
    "\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_test=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "indexes = np.arange(nx_train)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    datapoints = np.random.choice(indexes, size=batch_size, replace=False)\n",
    "    batch_x = xtrain[datapoints,:]\n",
    "    batch_y = ytrain[datapoints]\n",
    "    \n",
    "    batch_x_held_out = np.delete(xtrain, datapoints, axis=0)\n",
    "    batch_y_held_out = np.delete(ytrain, datapoints).reshape([batch_size_held_out,1])\n",
    "    \n",
    "    sig = lrf.sigmoid(batch_x@beta)\n",
    "    gradient = lrf.gradient_ols(batch_x,batch_y,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(batch_x@beta,batch_y.T)\n",
    "    cost_test = lrf.cost_log_ols(batch_x_held_out@beta,batch_y_held_out.T)\n",
    "    #Log Loss function from sklearn\n",
    "    if doprint:\n",
    "        logloss=log_loss(batch_y, np.round(batch_x@beta), eps=1e-16, normalize=True)\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_test)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_test.append(cost_test.ravel())\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_test, 'r')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.60444444444444 % Training Accuracy\n",
      "80.13333333333334 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.19111111111111 % Training Accuracy\n",
      "81.85333333333334 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "predicted_classes = model.predict(xtrain)\n",
    "accuracy = accuracy_score(ytrain.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytrain, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Training Accuracy')\n",
    "\n",
    "predicted_classes = model.predict(xtest)\n",
    "accuracy = accuracy_score(ytest.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytest, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) \n",
    "Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.X_data_full = X_data \n",
    "#self.Y_data_full = Y_data\n",
    "\n",
    "n_inputs, n_features = x.shape\n",
    "\n",
    "#self.epochs = epochs\n",
    "#self.batch_size = batch_size\n",
    "#self.iterations = self.n_inputs // self.batch_size\n",
    "#self.eta = eta\n",
    "#self.lmbd = lmbd\n",
    "\n",
    "\n",
    "n_hidden_neurons=50  # velge andre verdier om vi vil\n",
    "n_categories=2\n",
    "epochs=100\n",
    "batch_size=80\n",
    "eta=0.0001\n",
    "lmbd=0.1\n",
    "\n",
    "\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 50)\n",
      "(50,)\n",
      "(50, 2)\n",
      "(2,)\n",
      "(30000, 91)\n"
     ]
    }
   ],
   "source": [
    "print(hidden_weights.shape)\n",
    "print(hidden_bias.shape)\n",
    "print(output_weights.shape)\n",
    "print(output_bias.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_h = np.matmul(x, hidden_weights) + hidden_bias\n",
    "a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "\n",
    "exp_term = np.exp(z_o)\n",
    "probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 91)\n",
      "(30000, 50)\n",
      "(30000, 50)\n",
      "(30000, 2)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(z_h.shape)\n",
    "print(a_h.shape)\n",
    "print(z_o.shape)\n",
    "print(probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00298381, 0.99701619],\n",
       "       [0.69048859, 0.30951141],\n",
       "       [0.00576687, 0.99423313],\n",
       "       ...,\n",
       "       [0.00690587, 0.99309413],\n",
       "       [0.60833805, 0.39166195],\n",
       "       [0.00249443, 0.99750557]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
