{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "\n",
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import log_reg_functions as lrf\n",
    "import loaddata as ld\n",
    "\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ) \n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = ld.load_data(scaler='minmax')\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y, stratify=y, test_size=0.25, random_state= 0, shuffle=True) \n",
    "\n",
    "xtrain,xval,ytrain,yval = train_test_split(xtrain,ytrain, stratify=ytrain, test_size=0.25, random_state= 0, shuffle=True) \n",
    "# do this because want our test to be seperate, first splitting into test and train, and then splitting traindata \n",
    "#into training and validation\n",
    "\n",
    "nx_train, ny_train = xtrain.shape\n",
    "nx_test, ny_test = xtest.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(y == 1)\n",
    "indices_zero = np.where(y == 0)\n",
    "datapoints = np.random.choice(indices_zero[0], size=y[indices[0]].shape[0], replace=False)\n",
    "\n",
    "x_new = np.vstack((x[indices[0],:],x[datapoints,:]))\n",
    "y_new = np.vstack((y[indices[0]],y[datapoints]))\n",
    "\n",
    "xtrain = x_new\n",
    "ytrain = y_new\n",
    "nx_train, ny_train = xtrain.shape\n",
    "nx_test, ny_test = xtest.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) \n",
    "Egen logistic regression med gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05084800720214844\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = False\n",
    "doprint = False\n",
    "Niteration = 10\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_val=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = lrf.sigmoid(xtrain@beta)\n",
    "    gradient = lrf.gradient_ols(xtrain,ytrain,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(xtrain@beta,ytrain.T)\n",
    "    cost_val = lrf.cost_log_ols(xval@beta,yval.T) # do this for testdata at the end. \n",
    "    #Log Loss function from sklearn\n",
    "    if doprint:\n",
    "        logloss=log_loss(ytrain, np.round(xtrain@beta), eps=1e-16, normalize=True)\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_val)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_val.append(cost_val.ravel())\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_val, 'r')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()    \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "   \n",
    "# et relevant plot er iterativt plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3209 1172]\n",
      " [ 957  287]]\n"
     ]
    }
   ],
   "source": [
    "# making confusion matrix to check observed data with model predictions. \n",
    "\n",
    "predictions = xval@beta\n",
    "\n",
    "sig_val = np.round(lrf.sigmoid(predictions))\n",
    "\n",
    "cm = confusion_matrix(yval , sig_val.astype(int))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "Både egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.00331525015069 % Training Accuracy\n",
      "0.3143566815697963\n",
      "62.15111111111111 % Validation Accuracy\n",
      "0.21235664076951535\n",
      "61.373333333333335 % Test Accuracy\n",
      "0.21554291903601405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "print(f1_score(ytrain, classes, 'F1-score'))\n",
    "\n",
    "activation =lrf.sigmoid(xval@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==yval)/len(activation),'% Validation Accuracy')\n",
    "print(f1_score(yval, classes, 'F1-score'))\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n",
    "print(f1_score(ytest, classes, 'F1-score'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistisk regresjon med stokastisk gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.225665092468262\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = False\n",
    "doprint = False\n",
    "Niteration = 1000\n",
    "batch_size = 3000\n",
    "batch_size_held_out = nx_train-batch_size\n",
    "\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_test=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "indexes = np.arange(nx_train)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    datapoints = np.random.choice(indexes, size=batch_size, replace=False)\n",
    "    batch_x = xtrain[datapoints,:]\n",
    "    batch_y = ytrain[datapoints]\n",
    "    \n",
    "    batch_x_held_out = np.delete(xtrain, datapoints, axis=0)\n",
    "    batch_y_held_out = np.delete(ytrain, datapoints).reshape([batch_size_held_out,1])\n",
    "    \n",
    "    sig = lrf.sigmoid(batch_x@beta)\n",
    "    gradient = lrf.gradient_ols(batch_x,batch_y,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(batch_x@beta,batch_y.T)\n",
    "    cost_test = lrf.cost_log_ols(batch_x_held_out@beta,batch_y_held_out.T)\n",
    "    #Log Loss function from sklearn\n",
    "    if doprint:\n",
    "        logloss=log_loss(batch_y, np.round(batch_x@beta), eps=1e-16, normalize=True)\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_test)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_test.append(cost_test.ravel())\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_test, 'r')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.47287522603979 % Training Accuracy\n",
      "73.99111111111111 % Validation Accuracy\n",
      "73.65333333333334 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "\n",
    "activation =lrf.sigmoid(xval@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==yval)/len(activation),'% Validation Accuracy')\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.46413502109705 % Training Accuracy\n",
      "77.68 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "predicted_classes = model.predict(xtrain)\n",
    "accuracy = accuracy_score(ytrain.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytrain, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Training Accuracy')\n",
    "\n",
    "predicted_classes = model.predict(xtest)\n",
    "accuracy = accuracy_score(ytest.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytest, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) \n",
    "Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_data_full = X_data \n",
    "#Y_data_full = Y_data\n",
    " \n",
    "\n",
    "n_inputs, n_features = x.shape\n",
    "\n",
    "#epochs = epochs\n",
    "#batch_size = batch_size\n",
    "#iterations = self.n_inputs // self.batch_size\n",
    "#eta = eta\n",
    "#lmbd = lmbd\n",
    "\n",
    "\n",
    "n_hidden_neurons=50  \n",
    "n_categories=2\n",
    "epochs=100\n",
    "batch_size=80\n",
    "eta=0.0001\n",
    "lmbd=0.1\n",
    "\n",
    "# Creating weights and bias.  \n",
    "\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed forward\n",
    "z_h = np.matmul(x, hidden_weights) + hidden_bias\n",
    "a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "\n",
    "exp_term = np.exp(z_o)\n",
    "probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation\n",
    "\n",
    "#a_h, probabilities = feed_forward_train(X)\n",
    "    \n",
    "# error in the output layer\n",
    "error_output = probabilities - y\n",
    "# error in the hidden layer\n",
    "error_hidden = np.matmul(error_output, output_weights.T) * a_h * (1 - a_h)\n",
    "    \n",
    "# gradients for the output layer\n",
    "output_weights_gradient = np.matmul(a_h.T, error_output)\n",
    "output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    \n",
    "# gradient for the hidden layer\n",
    "hidden_weights_gradient = np.matmul(x.T, error_hidden)\n",
    "hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "if lmbd > 0.0:\n",
    "        output_weights_gradient += lmbd * output_weights\n",
    "        hidden_weights_gradient += lmbd * hidden_weights\n",
    "\n",
    "        output_weights -= eta * output_weights_gradient\n",
    "        output_bias -= eta * output_bias_gradient\n",
    "        hidden_weights -= eta * hidden_weights_gradient\n",
    "        hidden_bias -= eta * hidden_bias_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forsøk på Neural network: vil ha activation funksjon i flere lag, men må avslutte med sigmoid. Bruke ReLu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(x): \n",
    "    if self.activation_function == \"sigmoid\":\n",
    "      return 1.0/(1.0 + np.exp(-X))\n",
    "    elif self.activation_function == \"relu\":\n",
    "      return np.maximum(0,X)\n",
    "      \n",
    "def grad_activation(x):\n",
    "    if activation_function == \"sigmoid\":\n",
    "      return X*(1-X) \n",
    "    elif activation_function == \"relu\":\n",
    "      return 1.0*(X>0)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps, axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
