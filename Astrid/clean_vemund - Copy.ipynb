{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(5)\n",
    "\n",
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import log_reg_functions as lrf\n",
    "import loaddata as ld\n",
    "\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ) \n",
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = ld.load_data(scaler='minmax')\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size=0.25, random_state= 0, shuffle=True, stratify=y)\n",
    "\n",
    "xtrain,xval,ytrain,yval = train_test_split(xtrain,ytrain, test_size=0.25, random_state= 0, shuffle=True, stratify=ytrain)\n",
    "# do this because want our test to be seperate, first splitting into test and train, and then splitting traindata \n",
    "#into training and validation\n",
    "\n",
    "nx_train, ny_train = xtrain.shape\n",
    "nx_test, ny_test = xtest.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1-score is better when splitting data 50/50, this indicates that our algorithm learns more. The accuracy is expected to be lower for test data in this case. The dataset is biased, (read more) and therefore will give higher accuracy on biased datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = np.where(y == 1)\n",
    "indices_zero = np.where(y == 0)\n",
    "datapoints = np.random.choice(indices_zero[0], size=y[indices[0]].shape[0], replace=False)\n",
    "\n",
    "x_new = np.vstack((x[indices[0],:],x[datapoints,:]))\n",
    "y_new = np.vstack((y[indices[0]],y[datapoints]))\n",
    "\n",
    "xtrain = x_new\n",
    "ytrain = y_new\n",
    "\n",
    "nx_train, ny_train = xtrain.shape\n",
    "nx_test, ny_test = xtest.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) \n",
    "Egen logistic regression med gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.249404430389404\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = True\n",
    "doprint = False\n",
    "Niteration = 250\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_val=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = lrf.sigmoid(xtrain@beta)\n",
    "    gradient = lrf.gradient_ols(xtrain,ytrain,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(xtrain@beta,ytrain.T)\n",
    "    cost_val = lrf.cost_log_ols(xval@beta,yval.T) # do this for testdata at the end. \n",
    "    #Log Loss function from sklearn\n",
    "    logloss=log_loss(ytrain, np.round(xtrain@beta), eps=1e-16, normalize=True)\n",
    "    if doprint:\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_val)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_val.append(cost_val.ravel())\n",
    "        loglvec.append(logloss)\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_val, 'r')\n",
    "        #plt.plot(xaxis, loglvec, 'g')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()    \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "   \n",
    "# et relevant plot er iterativt plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4528 1313]\n",
      " [ 790  869]]\n"
     ]
    }
   ],
   "source": [
    "# making confusion matrix to check observed data with model predictions. \n",
    "\n",
    "predictions = xtest@beta\n",
    "sig_val = lrf.sigmoid(predictions)\n",
    "sig_val = np.round(sig_val)\n",
    "\n",
    "cm = confusion_matrix(ytest , sig_val.astype(int))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "BÃ¥de egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.70012055455094 % Training Accuracy\n",
      "0.5965033158212041\n",
      "72.05333333333333 % Validation Accuracy\n",
      "0.44919411352487737\n",
      "71.96 % Test Accuracy\n",
      "0.4524863316844571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "print(f1_score(ytrain, classes))\n",
    "\n",
    "activation =lrf.sigmoid(xval@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==yval)/len(activation),'% Validation Accuracy')\n",
    "print(f1_score(yval, classes))\n",
    "\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n",
    "print(f1_score(ytest, classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistisk regresjon med stokastisk gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.221460819244385\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = True\n",
    "doprint = False\n",
    "Niteration = 250\n",
    "batch_size = 3000\n",
    "batch_size_held_out = nx_train-batch_size\n",
    "\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_test=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "indexes = np.arange(nx_train)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    datapoints = np.random.choice(indexes, size=batch_size, replace=False)\n",
    "    batch_x = xtrain[datapoints,:]\n",
    "    batch_y = ytrain[datapoints]\n",
    "    \n",
    "    batch_x_held_out = np.delete(xtrain, datapoints, axis=0)\n",
    "    batch_y_held_out = np.delete(ytrain, datapoints).reshape([batch_size_held_out,1])\n",
    "    \n",
    "    sig = lrf.sigmoid(batch_x@beta)\n",
    "    gradient = lrf.gradient_ols(batch_x,batch_y,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(batch_x@beta,batch_y.T)\n",
    "    cost_test = lrf.cost_log_ols(batch_x_held_out@beta,batch_y_held_out.T)\n",
    "    #Log Loss function from sklearn\n",
    "    if doprint:\n",
    "        logloss=log_loss(batch_y, np.round(batch_x@beta), eps=1e-16, normalize=True)\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_test)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_test.append(cost_test.ravel())\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_test, 'r')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4382 1459]\n",
      " [ 732  927]]\n"
     ]
    }
   ],
   "source": [
    "# making confusion matrix to check observed data with model predictions. \n",
    "\n",
    "predictions = xtest@beta\n",
    "sig_val = lrf.sigmoid(predictions)\n",
    "sig_val = np.round(sig_val)\n",
    "\n",
    "cm = confusion_matrix(ytest , sig_val.astype(int))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.47703703703704 % Training Accuracy\n",
      "0.4567066521264994\n",
      "71.43111111111111 % Validation Accuracy\n",
      "0.4634390651085141\n",
      "70.78666666666666 % Test Accuracy\n",
      "0.45834363411619283\n"
     ]
    }
   ],
   "source": [
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "print(f1_score(ytrain, classes))\n",
    "\n",
    "activation =lrf.sigmoid(xval@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==yval)/len(activation),'% Validation Accuracy')\n",
    "print(f1_score(yval, classes))\n",
    "\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n",
    "print(f1_score(ytest, classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.26280892103676 % Training Accuracy\n",
      "77.36 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "predicted_classes = model.predict(xtrain)\n",
    "accuracy = accuracy_score(ytrain.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytrain, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Training Accuracy')\n",
    "\n",
    "predicted_classes = model.predict(xtest)\n",
    "accuracy = accuracy_score(ytest.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytest, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) \n",
    "Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.X_data_full = X_data \n",
    "#self.Y_data_full = Y_data\n",
    "\n",
    "n_inputs, n_features = x.shape\n",
    "\n",
    "#self.epochs = epochs\n",
    "#self.batch_size = batch_size\n",
    "#self.iterations = self.n_inputs // self.batch_size\n",
    "eta = 0.001\n",
    "lmbd = 10e-05\n",
    "\n",
    "\n",
    "n_hidden_neurons=50  # velge andre verdier om vi vil\n",
    "n_categories=2\n",
    "epochs=100\n",
    "batch_size=80\n",
    "eta=0.01\n",
    "n_hidden_neurons2=20  # velge andre verdier om vi vil\n",
    "\n",
    "\n",
    "\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_weights2 = np.random.randn(n_hidden_neurons, n_hidden_neurons2)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.1\n",
    "hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.1\n",
    "\n",
    "output_weights = np.random.randn(n_hidden_neurons2, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 50)\n",
      "(50,)\n",
      "(50, 20)\n",
      "(20,)\n",
      "(20, 2)\n",
      "(2,)\n",
      "(30000, 91)\n"
     ]
    }
   ],
   "source": [
    "print(hidden_weights.shape)\n",
    "print(hidden_bias.shape)\n",
    "print(hidden_weights2.shape)\n",
    "print(hidden_bias2.shape)\n",
    "print(output_weights.shape)\n",
    "print(output_bias.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_h = np.matmul(x, hidden_weights) + hidden_bias\n",
    "a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "z_h2 = np.matmul(a_h, hidden_weights2) + hidden_bias2\n",
    "a_h2 = lrf.sigmoid(z_h2)\n",
    "\n",
    "z_o = np.matmul(a_h2, output_weights) + output_bias\n",
    "\n",
    "exp_term = np.exp(z_o)\n",
    "probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 91)\n",
      "(30000, 50)\n",
      "(30000, 50)\n",
      "(30000, 20)\n",
      "(30000, 20)\n",
      "(30000, 2)\n",
      "[[0.86973101 0.13026899]\n",
      " [0.18130462 0.81869538]\n",
      " [0.16498562 0.83501438]\n",
      " ...\n",
      " [0.94564342 0.05435658]\n",
      " [0.03933096 0.96066904]\n",
      " [0.38076984 0.61923016]]\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(z_h.shape)\n",
    "print(a_h.shape)\n",
    "print(z_h2.shape)\n",
    "print(a_h2.shape)\n",
    "print(z_o.shape)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "error_output = probabilities - y\n",
    "error_hidden2 = np.matmul(error_output, output_weights.T) * a_h2 * (1 - a_h2)\n",
    "error_hidden = np.matmul(error_hidden2, hidden_weights2.T) * a_h * (1 - a_h) #a_h * (1 - a_h) is specific for sigmoid\n",
    "\n",
    "\n",
    "output_weights_gradient = np.matmul(a_h2.T, error_output)\n",
    "output_bias_gradient = np.sum(error_output, axis=0)\n",
    "\n",
    "hidden_weights_gradient2 = np.matmul(a_h.T, error_hidden2)\n",
    "hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)\n",
    "\n",
    "hidden_weights_gradient = np.matmul(x.T, error_hidden)\n",
    "hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "if lmbd > 0.0:\n",
    "    output_weights_gradient += lmbd * output_weights\n",
    "    hidden_weights_gradient2 += lmbd * hidden_weights2\n",
    "    hidden_weights_gradient += lmbd * hidden_weights\n",
    "    \n",
    "output_weights -= eta * output_weights_gradient\n",
    "output_bias -= eta * output_bias_gradient\n",
    "\n",
    "hidden_weights2 -= eta * hidden_weights_gradient2\n",
    "hidden_bias2 -= eta * hidden_bias_gradient2\n",
    "\n",
    "hidden_weights -= eta * hidden_weights_gradient\n",
    "hidden_bias -= eta * hidden_bias_gradient\n",
    "\n",
    "\n",
    "z_h = np.matmul(x, hidden_weights) + hidden_bias\n",
    "a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "z_h2 = np.matmul(a_h, hidden_weights2) + hidden_bias2\n",
    "a_h2 = lrf.sigmoid(z_h2)\n",
    "\n",
    "z_o = np.matmul(a_h2, output_weights) + output_bias\n",
    "\n",
    "exp_term = np.exp(z_o)\n",
    "probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan],\n",
       "       ...,\n",
       "       [nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "Niter = 1000\n",
    "x=xtrain\n",
    "y=ytrain\n",
    "\n",
    "n_inputs, n_features = x.shape\n",
    "\n",
    "#self.epochs = epochs\n",
    "#self.batch_size = batch_size\n",
    "#self.iterations = self.n_inputs // self.batch_size\n",
    "lmbd = 0.1\n",
    "\n",
    "\n",
    "n_hidden_neurons=20  # velge andre verdier om vi vil\n",
    "n_categories=2\n",
    "\n",
    "eta=0.0001\n",
    "n_hidden_neurons2=5  # velge andre verdier om vi vil\n",
    "\n",
    "\n",
    "\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_weights2 = np.random.randn(n_hidden_neurons, n_hidden_neurons2)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.1\n",
    "hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.1\n",
    "\n",
    "output_weights = np.random.randn(n_hidden_neurons2, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.1\n",
    "\n",
    "for i in range(Niter):\n",
    "    z_h = np.matmul(x, hidden_weights) + hidden_bias\n",
    "    a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "    z_h2 = np.matmul(a_h, hidden_weights2) + hidden_bias2\n",
    "    a_h2 = lrf.sigmoid(z_h2)\n",
    "\n",
    "    z_o = np.matmul(a_h2, output_weights) + output_bias\n",
    "\n",
    "    #probabilities = lrf.sigmoid(z_o)\n",
    "    exp_term = np.exp(z_o)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "\n",
    "    error_output = probabilities - y\n",
    "    error_hidden2 = np.matmul(error_output, output_weights.T) * a_h2 * (1 - a_h2)\n",
    "    error_hidden = np.matmul(error_hidden2, hidden_weights2.T) * a_h * (1 - a_h) #a_h * (1 - a_h) is specific for sigmoid\n",
    "\n",
    "\n",
    "    output_weights_gradient = np.matmul(a_h2.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "\n",
    "    hidden_weights_gradient2 = np.matmul(a_h.T, error_hidden2)\n",
    "    hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)\n",
    "\n",
    "    hidden_weights_gradient = np.matmul(x.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "    if lmbd > 0.0:\n",
    "        output_weights_gradient += lmbd * output_weights\n",
    "        hidden_weights_gradient2 += lmbd * hidden_weights2\n",
    "        hidden_weights_gradient += lmbd * hidden_weights\n",
    "\n",
    "    output_weights -= eta * output_weights_gradient\n",
    "    output_bias -= eta * output_bias_gradient\n",
    "\n",
    "    hidden_weights2 -= eta * hidden_weights_gradient2\n",
    "    hidden_bias2 -= eta * hidden_bias_gradient2\n",
    "\n",
    "    hidden_weights -= eta * hidden_weights_gradient\n",
    "    hidden_bias -= eta * hidden_bias_gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50007771 0.49992229]\n",
      " [0.50008078 0.49991922]\n",
      " [0.50007771 0.49992229]\n",
      " ...\n",
      " [0.50001634 0.49998366]\n",
      " [0.50001632 0.49998368]\n",
      " [0.50001632 0.49998368]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#print(hidden_weights)\n",
    "\n",
    "\n",
    "print((probabilities))\n",
    "print(np.isnan(probabilities).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.25828812537673 % Training Accuracy\n",
      "0.07519656897784131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes=np.argmax(probabilities,axis=1)\n",
    "print(100*np.sum(classes==ytrain.T)/len(ytrain),'% Training Accuracy')\n",
    "print(f1_score(ytrain, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=np.argmax(probabilities,axis=1)\n",
    "print(classes)\n",
    "ytrain.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
