{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "\n",
    "\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funksjoner\n",
    "Funksjoner som trenger å bli laget i dette dokumentet er følgende: \n",
    "\n",
    "- Cost funksjon til OLS - med gradient + logistic\n",
    "- Cost funksjon til Ridge - med gradient + logistic\n",
    "- Cost funksjon til Lasso - med gradient + logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Costfunctions:\n",
    "    def __init__(self, eta, lmd):\n",
    "        self.eta = eta\n",
    "        self.lmd = lmd\n",
    "        self.p = None\n",
    "\n",
    "    # Computes the residuals = error\n",
    "    def r(self, y): # resuduals.\n",
    "        temp =  y - self.p\n",
    "        return temp\n",
    "\n",
    "    def activation(self, Xw, key):\n",
    "        if (key == \"sigmoid\"):\n",
    "            self.p = 1. / (1. + np.exp(-Xw))\n",
    "            return self.p\n",
    "        elif(key == \"ELU\"):\n",
    "            if (Xw >= 0):\n",
    "                self.p = Xw\n",
    "                return self.p\n",
    "            else:\n",
    "                self.p = alpha*(np.exp(Xw)-1)\n",
    "                return self.p\n",
    "        else:\n",
    "            print(\"Unvalide keyword argument. Use siogmoid or ELU for activation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost_OLS(Costfunctions):\n",
    "\n",
    "    def __init__(self, eta, lmd = 0):\n",
    "        #self.w = w,\n",
    "        self.eta = eta,\n",
    "        self.lmd = lmd\n",
    "\n",
    "    \"\"\" Normal costfunction and gradient\"\"\"\n",
    "    def calculate_mse(self, X, y, w):\n",
    "        return (y-X.dot(w)).T@(y-X.dot(w))\n",
    "\n",
    "    def grad(self, X,y, w):\n",
    "        return 2/X.shape[1]* X.T.dot(w.dot(X)-y)\n",
    "\n",
    "    \"\"\" Logistic costfunction and gradient\"\"\"\n",
    "    def calculate_log(self, X, y,  w):\n",
    "        #self.p = self.activation(X, key)\n",
    "        return -y.dot(np.log(self.p+ 1e-12)) - ((1 - y).dot(np.log(1 - self.p + 1e-12)))\n",
    "\n",
    "    # returns a vector\n",
    "    def log_grad(self, X, w, errors):\n",
    "        # errora = y-p\n",
    "        return 1/X.shape[1]*X.T.dot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost_Ridge(Costfunctions):\n",
    "\n",
    "    def __init__(self, eta, lmd = 0.1):\n",
    "        #self.w = w,\n",
    "        self.eta = eta\n",
    "        self.lmd = lmd\n",
    "    \"\"\" Normal costfunction and gradient\"\"\"\n",
    "    def calculate_mse(self, X, y, w):\n",
    "        return (y-X.dot(w)).T@(y-X.dot(w)) + self.lmd*(np.sum(w)**2)\n",
    "\n",
    "    def grad(self, X, w, errors):\n",
    "        l2term =  self.lmd *np.sum(w** 2)\n",
    "        return - 2/X.shape[1]* X.T.dot( y - w.dot(X)) + l2term\n",
    "\n",
    "    \"\"\" Logistic costfunction and gradient\"\"\"\n",
    "    def calculate_log(self, X, y, w):\n",
    "        # penalty on bias to?\n",
    "        l2term =  self.lmd *np.sum(w ** 2)# + np.sum(w[0] ** 2))\n",
    "        return -y.dot(np.log(self.p+ 1e-12)) - ((1 - y).dot(np.log(1 - self.p+ 1e-12) )) + l2term\n",
    "\n",
    "    # returns a array\n",
    "    def log_grad(self, X, w, errors):\n",
    "        return 1/X.shape[1]*X.T.dot(errors) + 2*self.lmd*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost_Lasso(Costfunctions):\n",
    "\n",
    "    def __init__(self, eta, lmd):\n",
    "        #self.w = w,\n",
    "        self.eta = eta\n",
    "        self.lmd = lmd\n",
    "\n",
    "    \"\"\" Normal costfunction and gradient\"\"\"\n",
    "    def calculate_mse(self, X, y, w):\n",
    "        l1term = self.lmd*np.sum(np.abs(w))\n",
    "        return (y-X.dot(w)).T@(y-X.dot(w)) + l1term\n",
    "\n",
    "    def grad(self, X,y, w):\n",
    "        return - 2/X.shape[1]* X.T.dot( y - w.dot(X)) + self.lmd*np.sign(w)\n",
    "\n",
    "    \"\"\" Logistic costfunction and gradient\"\"\"\n",
    "    def calculate_log(self, X, y, w):\n",
    "        # returns a scalar.\n",
    "        #self.p = self.activation(X, \"sigmoid\")\n",
    "        l1term = self.lmd*np.sum(np.abs(w))\n",
    "        return -y.dot(np.log(self.p+ 1e-12)) - ((1 - y).dot(np.log(1 - self.p+1e-12))) + l1term\n",
    "\n",
    "    # The derivative of a absolute value is a signfunction\n",
    "    def log_grad(self, X, w, errors):\n",
    "        return 1/X.shape[1]*X.T.dot(errors) + self.lmd*np.sign(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
