{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #return np.exp(prediction) / (1. + np.exp(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_solver(N, eta, design, data, beta=None):\n",
    "    M=len(data)\n",
    "    if beta != None:\n",
    "        beta = beta\n",
    "    else:\n",
    "        beta = np.random.randn(design.shape[1])\n",
    "     \n",
    "    for i in range(N):\n",
    "        gradients = cost_grad_ols(design,frank,beta)\n",
    "        beta -= eta*gradients\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loaddata as ld\n",
    "\n",
    "X,y = ld.load_data(scaler='minmax')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost [[1.95375457]] & Log loss 23.777812541919044\n",
      "Cost [[1.83905812]] & Log loss 23.591937335964303\n",
      "Cost [[1.73223892]] & Log loss 23.176005173180876\n",
      "Cost [[1.63365266]] & Log loss 22.505629414107197\n",
      "Cost [[1.54350635]] & Log loss 22.221897452915492\n",
      "Cost [[1.46182741]] & Log loss 21.42780627828449\n",
      "Cost [[1.38845303]] & Log loss 20.61043452183692\n",
      "Cost [[1.32304156]] & Log loss 19.903095413537404\n",
      "Cost [[1.26510253]] & Log loss 18.562899265532373\n",
      "Cost [[1.21403854]] & Log loss 17.7687000446192\n",
      "Cost [[1.16919048]] & Log loss 17.37845575932302\n",
      "Cost [[1.12987947]] & Log loss 16.345491732254533\n",
      "Cost [[1.09544075]] & Log loss 14.888990265357053\n",
      "Cost [[1.06524792]] & Log loss 14.300395139896143\n",
      "Cost [[1.03872789]] & Log loss 13.950564820590545\n",
      "Cost [[1.01536803]] & Log loss 13.604240883872752\n",
      "Cost [[0.99471758]] & Log loss 13.27016951807273\n",
      "Cost [[0.97638523]] & Log loss 13.083031413095654\n",
      "Cost [[0.96003446]] & Log loss 12.824882768472968\n",
      "Cost [[0.94537782]] & Log loss 12.646316583628819\n",
      "Cost [[0.93217089]] & Log loss 12.379540253050154\n",
      "Cost [[0.92020641]] & Log loss 11.943802522762498\n",
      "Cost [[0.90930898]] & Log loss 11.231373479370273\n",
      "Cost [[0.89933017]] & Log loss 11.020892056025701\n",
      "Cost [[0.89014437]] & Log loss 10.956126304212967\n",
      "Cost [[0.88164521]] & Log loss 10.892553744143754\n",
      "Cost [[0.87374248]] & Log loss 10.864507366080995\n",
      "Cost [[0.86635953]] & Log loss 10.825398123480038\n",
      "Cost [[0.85943114]] & Log loss 10.781352301799778\n",
      "Cost [[0.85290167]] & Log loss 10.733688565885602\n",
      "Cost [[0.84672356]] & Log loss 10.697028443322626\n",
      "Cost [[0.84085605]] & Log loss 10.665301414475012\n",
      "Cost [[0.83526413]] & Log loss 10.639686729630526\n",
      "Cost [[0.82991767]] & Log loss 10.605437388102176\n",
      "Cost [[0.82479063]] & Log loss 10.582226513564438\n",
      "Cost [[0.81986052]] & Log loss 10.556552577532957\n",
      "Cost [[0.8151078]] & Log loss 10.512489329032988\n",
      "Cost [[0.81051548]] & Log loss 10.492931222368568\n",
      "Cost [[0.80606877]] & Log loss 10.453787126128203\n",
      "Cost [[0.80175473]] & Log loss 10.414632573796018\n",
      "Cost [[0.79756203]] & Log loss 10.392604434909975\n",
      "Cost [[0.79348073]] & Log loss 10.368144602805657\n",
      "Cost [[0.78950207]] & Log loss 10.355923400163352\n",
      "Cost [[0.78561834]] & Log loss 10.329000506565293\n",
      "Cost [[0.7818227]] & Log loss 10.299618036837433\n",
      "Cost [[0.77810911]] & Log loss 10.276361852568462\n",
      "Cost [[0.77447221]] & Log loss 10.26781084461918\n",
      "Cost [[0.7709072]] & Log loss 10.247014236480009\n",
      "Cost [[0.76740982]] & Log loss 10.213958086695182\n",
      "Cost [[0.76397626]] & Log loss 10.19316147855601\n",
      "Cost [[0.76060308]] & Log loss 10.158891224844016\n",
      "Cost [[0.75728722]] & Log loss 10.139315691359894\n",
      "Cost [[0.75402591]] & Log loss 10.116062992454863\n",
      "Cost [[0.75081664]] & Log loss 10.064645413113078\n",
      "Cost [[0.74765716]] & Log loss 10.035280370204923\n",
      "Cost [[0.7445454]] & Log loss 9.977743476132133\n",
      "Cost [[0.74147949]] & Log loss 9.928785472920149\n",
      "Cost [[0.73845774]] & Log loss 9.901859093958151\n",
      "Cost [[0.73547859]] & Log loss 9.867606267065861\n",
      "Cost [[0.7325406]] & Log loss 9.816171260904373\n",
      "Cost [[0.72964247]] & Log loss 9.76717840405298\n",
      "Cost [[0.72678298]] & Log loss 9.732908150340986\n",
      "Cost [[0.72396103]] & Log loss 9.69986245664798\n",
      "Cost [[0.72117557]] & Log loss 9.666809792227093\n",
      "Cost [[0.71842564]] & Log loss 9.630055564837713\n",
      "Cost [[0.71571036]] & Log loss 9.606785439112976\n",
      "Cost [[0.71302888]] & Log loss 9.581062707986321\n",
      "Cost [[0.71038043]] & Log loss 9.562708249157245\n",
      "Cost [[0.70776428]] & Log loss 9.54189072883443\n",
      "Cost [[0.70517974]] & Log loss 9.516167997707774\n",
      "Cost [[0.70262616]] & Log loss 9.491694224147693\n",
      "Cost [[0.70010293]] & Log loss 9.468438039878722\n",
      "Cost [[0.69760948]] & Log loss 9.430480164653995\n",
      "Cost [[0.69514525]] & Log loss 9.399880105635027\n",
      "Cost [[0.69270972]] & Log loss 9.36436786508434\n",
      "Cost [[0.6903024]] & Log loss 9.32274328053053\n",
      "Cost [[0.68792281]] & Log loss 9.279890650593787\n",
      "Cost [[0.6855705]] & Log loss 9.22477847901139\n",
      "Cost [[0.68324504]] & Log loss 9.199069689340497\n",
      "Cost [[0.68094602]] & Log loss 9.173360899669605\n",
      "Cost [[0.67867302]] & Log loss 9.14641360852396\n",
      "Cost [[0.67642568]] & Log loss 9.114568077302357\n",
      "Cost [[0.67420362]] & Log loss 9.093757527707425\n",
      "Cost [[0.67200648]] & Log loss 9.069262841963699\n",
      "Cost [[0.66983393]] & Log loss 9.063157468688456\n",
      "Cost [[0.66768562]] & Log loss 9.038655812216849\n",
      "Cost [[0.66556124]] & Log loss 9.017848747985854\n",
      "Cost [[0.66346047]] & Log loss 8.998245331590203\n",
      "Cost [[0.66138302]] & Log loss 8.982336507435166\n",
      "Cost [[0.65932858]] & Log loss 8.967631331115472\n",
      "Cost [[0.65729688]] & Log loss 8.945582280045784\n",
      "Cost [[0.65528764]] & Log loss 8.910042156583568\n",
      "Cost [[0.65330058]] & Log loss 8.87697555070692\n",
      "Cost [[0.65133545]] & Log loss 8.853722851801887\n",
      "Cost [[0.64939199]] & Log loss 8.823105365963213\n",
      "Cost [[0.64746994]] & Log loss 8.785130063918784\n",
      "Cost [[0.64556907]] & Log loss 8.750831927295264\n",
      "Cost [[0.64368914]] & Log loss 8.730010921608507\n",
      "Cost [[0.64182991]] & Log loss 8.701849526535694\n",
      "Cost [[0.63999115]] & Log loss 8.671239011424904\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # This is out eta\n",
    "\n",
    "Niteration = 100\n",
    "beta = np.random.randn(X.shape[1],1)\n",
    "#\n",
    "#beta = parameters.reshape([26,1])\n",
    "\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = sigmoid(X@beta)\n",
    "    gradient = np.dot(X.T, (sig - y)) / y.shape[0]\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    \n",
    "    #Cost function\n",
    "    cost = cost_log_ols(X@beta,y.T)\n",
    "    # Log Loss function from sklearn\n",
    "    logloss=log_loss(y, np.round(X@beta), eps=1e-16, normalize=True)\n",
    "    print('Cost', cost,'&','Log loss', logloss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "BÃ¥de egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.3 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation =sigmoid(X@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==y)/len(activation),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.15 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(y, predicted_classes)\n",
    "\n",
    "print(accuracy, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
