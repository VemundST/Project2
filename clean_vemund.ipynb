{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import log_reg_functions as lrf\n",
    "import loaddata as ld\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X,y = ld.load_data(scaler='minmax')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost [[0.7561022]] & Log loss 9.184756071057912\n",
      "Cost [[0.75265803]] & Log loss 9.173696691883658\n",
      "Cost [[0.74928295]] & Log loss 9.16875314207647\n",
      "Cost [[0.74597568]] & Log loss 9.168739200620706\n",
      "Cost [[0.74273492]] & Log loss 9.166279624490906\n",
      "Cost [[0.73955937]] & Log loss 9.1662621976712\n",
      "Cost [[0.73644772]] & Log loss 9.15888695464574\n",
      "Cost [[0.73339865]] & Log loss 9.140466273901787\n",
      "Cost [[0.73041086]] & Log loss 9.138006697771985\n",
      "Cost [[0.72748302]] & Log loss 9.131873441585215\n",
      "Cost [[0.72461381]] & Log loss 9.128185820072483\n",
      "Cost [[0.72180194]] & Log loss 9.112235171550155\n",
      "Cost [[0.7190461]] & Log loss 9.093814490806203\n",
      "Cost [[0.716345]] & Log loss 9.074197132954788\n",
      "Cost [[0.71369734]] & Log loss 9.069281466059127\n",
      "Cost [[0.71110187]] & Log loss 9.069274495331246\n",
      "Cost [[0.70855733]] & Log loss 9.05210625751781\n",
      "Cost [[0.70606247]] & Log loss 9.036159094359423\n",
      "Cost [[0.70361606]] & Log loss 9.023878640530123\n",
      "Cost [[0.70121691]] & Log loss 8.998124541127998\n",
      "Cost [[0.69886383]] & Log loss 8.989556106359013\n",
      "Cost [[0.69655564]] & Log loss 8.985882426302046\n",
      "Cost [[0.69429119]] & Log loss 8.976078975422249\n",
      "Cost [[0.69206937]] & Log loss 8.968735100672255\n",
      "Cost [[0.68988906]] & Log loss 8.95770708977347\n",
      "Cost [[0.68774919]] & Log loss 8.93932823339681\n",
      "Cost [[0.68564869]] & Log loss 8.93197390255499\n",
      "Cost [[0.68358654]] & Log loss 8.918493286254288\n",
      "Cost [[0.68156172]] & Log loss 8.907468760719443\n",
      "Cost [[0.67957325]] & Log loss 8.885409253557931\n",
      "Cost [[0.67762017]] & Log loss 8.878061893443997\n",
      "Cost [[0.67570153]] & Log loss 8.860918053178148\n",
      "Cost [[0.67381643]] & Log loss 8.847433951513505\n",
      "Cost [[0.67196398]] & Log loss 8.813121873434218\n",
      "Cost [[0.67014331]] & Log loss 8.780034355373921\n",
      "Cost [[0.66835358]] & Log loss 8.756746802829479\n",
      "Cost [[0.66659397]] & Log loss 8.74815745587685\n",
      "Cost [[0.6648637]] & Log loss 8.730989218063415\n",
      "Cost [[0.66316199]] & Log loss 8.719961207164632\n",
      "Cost [[0.66148809]] & Log loss 8.705249060117055\n",
      "Cost [[0.65984128]] & Log loss 8.6954490946012\n",
      "Cost [[0.65822085]] & Log loss 8.684417598338474\n",
      "Cost [[0.65662613]] & Log loss 8.668477405907968\n",
      "Cost [[0.65505645]] & Log loss 8.658666984300291\n",
      "Cost [[0.65351118]] & Log loss 8.650084608075545\n",
      "Cost [[0.65198968]] & Log loss 8.63292334098999\n",
      "Cost [[0.65049137]] & Log loss 8.626797055531103\n",
      "Cost [[0.64901566]] & Log loss 8.616976177831603\n",
      "Cost [[0.64756199]] & Log loss 8.604709665458065\n",
      "Cost [[0.64612981]] & Log loss 8.601025529309274\n",
      "Cost [[0.6447186]] & Log loss 8.59733093706866\n",
      "Cost [[0.64332784]] & Log loss 8.588734619388148\n",
      "Cost [[0.64195705]] & Log loss 8.583815467128547\n",
      "Cost [[0.64060573]] & Log loss 8.575229605539858\n",
      "Cost [[0.63927344]] & Log loss 8.560503517036521\n",
      "Cost [[0.63795972]] & Log loss 8.55804394090672\n",
      "Cost [[0.63666415]] & Log loss 8.544556353878132\n",
      "Cost [[0.63538629]] & Log loss 8.54209329238439\n",
      "Cost [[0.63412574]] & Log loss 8.532282870776713\n",
      "Cost [[0.63288212]] & Log loss 8.527381145336816\n",
      "Cost [[0.63165504]] & Log loss 8.513883102216404\n",
      "Cost [[0.63044413]] & Log loss 8.50408313670055\n",
      "Cost [[0.62924904]] & Log loss 8.496728805858734\n",
      "Cost [[0.62806942]] & Log loss 8.490599035035904\n",
      "Cost [[0.62690493]] & Log loss 8.488146429633986\n",
      "Cost [[0.62575525]] & Log loss 8.474644901149636\n",
      "Cost [[0.62462008]] & Log loss 8.468511644962867\n",
      "Cost [[0.62349909]] & Log loss 8.46360643415903\n",
      "Cost [[0.622392]] & Log loss 8.45502057257034\n",
      "Cost [[0.62129853]] & Log loss 8.451329465693668\n",
      "Cost [[0.62021839]] & Log loss 8.450094449582856\n",
      "Cost [[0.61915131]] & Log loss 8.442733148013156\n",
      "Cost [[0.61809703]] & Log loss 8.436596406462446\n",
      "Cost [[0.61705531]] & Log loss 8.428010544873759\n",
      "Cost [[0.61602588]] & Log loss 8.419417712557188\n",
      "Cost [[0.61500852]] & Log loss 8.418189667174257\n",
      "Cost [[0.61400299]] & Log loss 8.412052925623549\n",
      "Cost [[0.61300907]] & Log loss 8.403463578670918\n",
      "Cost [[0.61202653]] & Log loss 8.402225077196166\n",
      "Cost [[0.61105517]] & Log loss 8.396084850281516\n",
      "Cost [[0.61009477]] & Log loss 8.392393743404842\n",
      "Cost [[0.60914514]] & Log loss 8.392390258040901\n",
      "Cost [[0.60820607]] & Log loss 8.392369345857256\n",
      "Cost [[0.60727738]] & Log loss 8.391134329746444\n",
      "Cost [[0.60635888]] & Log loss 8.384994102831794\n",
      "Cost [[0.60545038]] & Log loss 8.386229118942605\n",
      "Cost [[0.60455173]] & Log loss 8.382555438885637\n",
      "Cost [[0.60366273]] & Log loss 8.37642915342675\n",
      "Cost [[0.60278323]] & Log loss 8.359274857069078\n",
      "Cost [[0.60191306]] & Log loss 8.355583750192405\n",
      "Cost [[0.60105206]] & Log loss 8.359260915613314\n",
      "Cost [[0.60020008]] & Log loss 8.355583750192407\n",
      "Cost [[0.59935696]] & Log loss 8.345769843220788\n",
      "Cost [[0.59852256]] & Log loss 8.33595593624917\n",
      "Cost [[0.59769674]] & Log loss 8.32491049853068\n",
      "Cost [[0.59687935]] & Log loss 8.3200018023629\n",
      "Cost [[0.59607026]] & Log loss 8.31386854617613\n",
      "Cost [[0.59526932]] & Log loss 8.301619460622298\n",
      "Cost [[0.59447642]] & Log loss 8.301615975258358\n",
      "Cost [[0.59369143]] & Log loss 8.296717735182401\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # This is out eta\n",
    "\n",
    "Niteration = 100\n",
    "beta = np.random.randn(X.shape[1],1)\n",
    "\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = lrf.sigmoid(X@beta)\n",
    "    gradient = lrf.gradient_ols(X,y,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(X@beta,y.T)\n",
    "    # Log Loss function from sklearn\n",
    "    logloss=log_loss(y, np.round(X@beta), eps=1e-16, normalize=True)\n",
    "    print('Cost', cost,'&','Log loss', logloss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "BÃ¥de egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.3 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation =sigmoid(X@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==y)/len(activation),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.15 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(y, predicted_classes)\n",
    "\n",
    "print(accuracy, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
