{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(prediction):\n",
    "    '''\n",
    "    Sigmoid activation function\n",
    "    '''\n",
    "    return 1. / (1. + np.exp(-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_(prediction):\n",
    "    '''\n",
    "    Relu activation function\n",
    "    '''\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mse_ols(design, data, beta):\n",
    "    '''\n",
    "    Mean squared error\n",
    "    '''\n",
    "    return (data - design.dot(beta)).T*(data - design.dot(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_ols(design, data, beta):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) #logistic regression slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_log_ols(design, data, beta):\n",
    "    '''\n",
    "    Logisitic regression cost function\n",
    "    '''\n",
    "    return -data.dot(np.log(prediction)+1e-10) - ((1-data).dot(np.log(1-prediction + 1e-10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_log_ols(design, data, p):\n",
    "    '''\n",
    "    Gradient w.r.t log\n",
    "    '''\n",
    "    return (1/len(data))*design.T.dot(data-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_mse_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Mean squared error\n",
    "    '''\n",
    "    return (data - design.dot(beta)).T*(data - design.dot(beta)) + _lambda(np.sum(beta)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    regu_term = _lambda*np.sum(beta**2) \n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) + _lambda*np.sum(beta**2) + regu_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_log_rid(design, data, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Logisitic regression cost function\n",
    "    '''\n",
    "    regu_term = _lambda*np.sum(beta**2) \n",
    "    return -data.dot(np.log(prediction)+1e-10) - ((1-data).dot(np.log(1-prediction + 1e-10))) + regu_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad_log_rid(design, data, p, beta, _lambda=1e-07):\n",
    "    '''\n",
    "    Gradient w.r.t log\n",
    "    '''\n",
    "    return (1/len(data))*design.T.dot(data-p) +2*_lambda*beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_solver(N, eta, design, data, beta=None):\n",
    "    M=len(data)\n",
    "    if beta != None:\n",
    "        beta = beta\n",
    "    else:\n",
    "        beta = np.random.randn(design.shape[1])\n",
    "     \n",
    "    for i in range(N):\n",
    "        gradients = cost_grad_ols(design,frank,beta)\n",
    "        beta -= eta*gradients\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_x         = 50\n",
    "x           = np.linspace(0, 1, n_x)\n",
    "y           = np.linspace(0, 1, n_x)\n",
    "\n",
    "x_mesh, y_mesh  = np.meshgrid(x,y)\n",
    "noise_level     = 0.01\n",
    "frank           = fx.FrankeFunction(x_mesh, y_mesh, noise_level)\n",
    "\n",
    "frank = np.ravel(frank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = fx.DesignDesign(x,y,10)\n",
    "data = frank.reshape([n_x*n_x,1])\n",
    "np.random.seed(2018)\n",
    "M=len(data)\n",
    "N=10000\n",
    "eta=0.1\n",
    "\n",
    "beta = gradient_solver(N, eta, design, data)\n",
    "\n",
    "\n",
    "prediction = design @ beta\n",
    "pred = np.reshape(prediction,[n_x,n_x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "# Trying to set the seed\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Reading file into data frame\n",
    "directory = os.getcwd()\n",
    "filename = directory + '/cred_card.xls'\n",
    "nanDict = {} # fjerner NaN \n",
    "dataframe = pd.read_excel(filename, header=1, skiprows=0, index_col=0, na_values=nanDict)\n",
    "\n",
    "\n",
    "dataframe.rename(index=str, columns={\"default payment next month\": \"defaultPaymentNextMonth\"}, inplace=True)\n",
    "\n",
    "# Features and targets \n",
    "X = dataframe.loc[:, dataframe.columns != 'defaultPaymentNextMonth'].values\n",
    "y = dataframe.loc[:, dataframe.columns == 'defaultPaymentNextMonth'].values\n",
    "\n",
    "# Categorical variables to one-hot's\n",
    "onehotencoder = OneHotEncoder(categories=\"auto\")\n",
    "\n",
    "X = ColumnTransformer(\n",
    "    [(\"\", onehotencoder, [3]),],\n",
    "    remainder=\"passthrough\"\n",
    ").fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "trainingShare = 0.5 \n",
    "seed  = 1\n",
    "XTrain, XTest, yTrain, yTest=train_test_split(X, y, train_size=trainingShare, \\\n",
    "                                              test_size = 1-trainingShare,\n",
    "                                             random_state=seed)\n",
    "\n",
    "# Input Scaling\n",
    "sc = StandardScaler()\n",
    "XTrain = sc.fit_transform(XTrain)\n",
    "XTest = sc.transform(XTest)\n",
    "\n",
    "# One-hot's of the target vector\n",
    "Y_train_onehot, Y_test_onehot = onehotencoder.fit_transform(yTrain), onehotencoder.fit_transform(yTest)\n",
    "\n",
    "\n",
    "# Remove instances with zeros only for past bill statements or paid amounts\n",
    "'''\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.BILL_AMT1 == 0) &\n",
    "                (dataframe.BILL_AMT2 == 0) &\n",
    "                (dataframe.BILL_AMT3 == 0) &\n",
    "                (dataframe.BILL_AMT4 == 0) &\n",
    "                (dataframe.BILL_AMT5 == 0) &\n",
    "                (dataframe.BILL_AMT6 == 0) &\n",
    "                (dataframe.PAY_AMT1 == 0) &\n",
    "                (dataframe.PAY_AMT2 == 0) &\n",
    "                (dataframe.PAY_AMT3 == 0) &\n",
    "                (dataframe.PAY_AMT4 == 0) &\n",
    "                (dataframe.PAY_AMT5 == 0) &\n",
    "                (dataframe.PAY_AMT6 == 0)].index)\n",
    "'''\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.BILL_AMT1 == 0) &\n",
    "                (dataframe.BILL_AMT2 == 0) &\n",
    "                (dataframe.BILL_AMT3 == 0) &\n",
    "                (dataframe.BILL_AMT4 == 0) &\n",
    "                (dataframe.BILL_AMT5 == 0) &\n",
    "                (dataframe.BILL_AMT6 == 0)].index)\n",
    "\n",
    "dataframe = dataframe.drop(dataframe[(dataframe.PAY_AMT1 == 0) &\n",
    "                (dataframe.PAY_AMT2 == 0) &\n",
    "                (dataframe.PAY_AMT3 == 0) &\n",
    "                (dataframe.PAY_AMT4 == 0) &\n",
    "                (dataframe.PAY_AMT5 == 0) &\n",
    "                (dataframe.PAY_AMT6 == 0)].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#lambdas=np.logspace(-5,7,13)\n",
    "#parameters = [{'C': 1./lambdas, \"solver\":[\"lbfgs\"]}]#*len(parameters)}]\n",
    "#scoring = ['accuracy', 'roc_auc']\n",
    "#logReg = LogisticRegression()\n",
    "#gridSearch = GridSearchCV(logReg, parameters, cv=5, scoring=scoring, refit='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "y=np.squeeze(y)\n",
    "logreg = SGDClassifier(max_iter = 100000, penalty=None, eta0=0.1, learning_rate='constant' )\n",
    "logreg.fit(X,y)\n",
    "prediction = logreg.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.count_nonzero(y-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.79 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (len(y)-count)/len(y)* 100\n",
    "print(accuracy, '%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistic regression.\n",
    "Ulik prøving med dif deffinisjoner.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Activation function used to map any real value between 0 and 1\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def net_input(eta, x):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(x, eta)\n",
    "\n",
    "def probability(eta, x):\n",
    "    '''\n",
    "    Returns the probability after passing through sigmoid\n",
    "    '''\n",
    "    return sigmoid(net_input(eta, x))\n",
    "\n",
    "\n",
    "def cost_grad_ols(design, data, beta):\n",
    "    '''\n",
    "    Calculates the first derivative of MSE w.r.t beta.\n",
    "    '''\n",
    "    return (2/len(data))*design.T.dot(design.dot(beta)-data) #logistic regression slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is nan\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is inf\n",
      "cost is 15246.062682755179\n",
      "cost is 14081.252898542029\n",
      "cost is 13226.557692386417\n",
      "cost is 12574.541385602412\n",
      "cost is 12060.231757364547\n",
      "cost is 11644.483430307546\n",
      "cost is 11301.443850104868\n",
      "cost is 11013.469629338791\n",
      "cost is 10768.735492955668\n",
      "cost is 10559.611693112962\n",
      "cost is 10381.463321176021\n",
      "cost is 10231.745149905633\n",
      "cost is 10109.121025145292\n",
      "cost is 10012.370406455138\n",
      "cost is 9939.260714541333\n",
      "cost is 9885.963371712649\n",
      "cost is 9847.644097653047\n",
      "cost is 9819.857982081507\n",
      "cost is 9799.359922388121\n",
      "cost is 9784.006586555477\n",
      "cost is 9772.378499999038\n",
      "cost is 9763.487207875041\n",
      "cost is 9756.610621817119\n",
      "cost is 9751.209876967348\n",
      "cost is 9746.884397208962\n",
      "cost is 9743.341600851041\n",
      "cost is 9740.372300618894\n",
      "cost is 9737.829647949862\n",
      "cost is 9735.611666729686\n",
      "cost is 9733.647569860705\n",
      "cost is 9731.887677153507\n",
      "cost is 9730.296365702223\n",
      "cost is 9728.847341407205\n",
      "cost is 9727.520552704438\n",
      "cost is 9726.300207595552\n",
      "cost is 9725.173502879192\n",
      "cost is 9724.129804263628\n",
      "cost is 9723.160107722497\n",
      "cost is 9722.256676936868\n",
      "cost is 9721.412791101999\n",
      "cost is 9720.62256361766\n",
      "cost is 9719.880806719271\n",
      "cost is 9719.182927324371\n",
      "cost is 9718.5248441697\n",
      "cost is 9717.902920598073\n",
      "cost is 9717.313908571654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  from ipykernel import kernelapp as app\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in matmul\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is 9716.754901694712\n",
      "cost is 9716.223294896077\n",
      "cost is 9715.716749947838\n",
      "cost is 9715.233165285537\n",
      "cost is 9714.770649954986\n",
      "cost is 9714.327500476371\n",
      "cost is 9713.902180815468\n",
      "cost is 9713.493304367177\n",
      "cost is 9713.099618394695\n",
      "cost is 9712.71998983806\n",
      "cost is 9712.353393147423\n",
      "cost is 9711.99889899861\n",
      "cost is 9711.655664748258\n",
      "cost is 9711.322925382023\n",
      "cost is 9710.999986021427\n",
      "cost is 9710.686214597232\n",
      "cost is 9710.3810359799\n",
      "cost is 9710.08392598957\n",
      "cost is 9709.794406825295\n",
      "cost is 9709.512042110171\n",
      "cost is 9709.236433371163\n",
      "cost is 9708.967215883236\n",
      "cost is 9708.704056009154\n",
      "cost is 9708.446647656656\n",
      "cost is 9708.194710330703\n",
      "cost is 9707.947986057767\n",
      "cost is 9707.70623803433\n",
      "cost is 9707.469247906547\n",
      "cost is 9707.236814919077\n",
      "cost is 9707.008753471418\n",
      "cost is 9706.784892679183\n",
      "cost is 9706.565074166105\n",
      "cost is 9706.349151941527\n",
      "cost is 9706.136990438472\n",
      "cost is 9705.928464577459\n",
      "cost is 9705.723458140761\n",
      "cost is 9705.521863816884\n",
      "cost is 9705.323582134433\n",
      "cost is 9705.12852113241\n",
      "cost is 9704.936596308391\n",
      "cost is 9704.747729280949\n",
      "cost is 9704.561849614362\n",
      "cost is 9704.37889132955\n",
      "cost is 9704.198798190944\n",
      "cost is 9704.021515999\n",
      "cost is 9703.847004213247\n",
      "cost is 9703.675220295787\n",
      "cost is 9703.506142842985\n"
     ]
    }
   ],
   "source": [
    "eta = 0.0001 # This is out eta\n",
    "m = 10\n",
    "\n",
    "Niteration = 100\n",
    "beta = np.random.randn(26,1)\n",
    "for iter in range(Niteration):\n",
    "    sigmoid = 1/(1+np.exp(-(XTrain)@(beta))) # vi har ikke definert prediction i vår sigmoid definisjon. \n",
    "    gradients = -(np.transpose(XTrain)@(yTrain-sigmoid))\n",
    "    beta -= eta*gradients\n",
    "  \n",
    "    # Cost function\n",
    "    \n",
    "    #total_cost = -(1 / m) * np.sum(y @ np.log(sigmoid(X))) + (1 - y) @ np.log(1 - sigmoid(X)))\n",
    "    \n",
    "    cost = -np.sum(np.transpose(yTrain)@np.log(sigmoid) + np.transpose(1-yTrain)@np.log(1-sigmoid))\n",
    "    print('cost is', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "Både egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "accuracy() missing 1 required positional argument: 'actual_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-7f82a97d0e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mactual_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: accuracy() missing 1 required positional argument: 'actual_classes'"
     ]
    }
   ],
   "source": [
    "def predict(self, x):\n",
    "    theta = parameters[:, np.newaxis]\n",
    "    return probability(theta, x)\n",
    "\n",
    "def accuracy(self, x, actual_classes, probab_threshold=0.5):\n",
    "    predicted_classes = (predict(x) >= \n",
    "                         probab_threshold).astype(int)\n",
    "    predicted_classes = predicted_classes.flatten()\n",
    "    accuracy = np.mean(predicted_classes == actual_classes)\n",
    "    return accuracy * 100\n",
    "accuracy(X, y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.87333333333333 %\n"
     ]
    }
   ],
   "source": [
    "#print(parameters)\n",
    "print(accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips fra gruppelærer: \n",
    "Lage et enklere dataset som har x med 1000 elementer og første 500 verdiene = 0 og de siste etter = 1, slik at y fra 0 - 500 = 0 osv. Sjekker man logisitic regression på dette så vil accuracy være 100% med scikit. Kan bruke cost-funksjon med log for OLS. Ikke nødvenig å kjøre for Ridge og Lasso, dette er tidskrevende for oppgaven. Han tror ikke vi har tid til dette. Var inne på god tanke med loopen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
