{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_class as fx\n",
    "import classx as cl\n",
    "import log_reg_functions as lrf\n",
    "import loaddata as ld\n",
    "\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = ld.load_data(scaler='minmax')\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "nx_train, ny_train = xtrain.shape\n",
    "nx_test, ny_test = xtest.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistic regression med gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.287982225418091\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = False\n",
    "doprint = False\n",
    "Niteration = 1000\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_test=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    \n",
    "    sig = lrf.sigmoid(xtrain@beta)\n",
    "    gradient = lrf.gradient_ols(xtrain,ytrain,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(xtrain@beta,ytrain.T)\n",
    "    cost_test = lrf.cost_log_ols(xtest@beta,ytest.T)\n",
    "    #Log Loss function from sklearn\n",
    "    if doprint:\n",
    "        logloss=log_loss(ytrain, np.round(xtrain@beta), eps=1e-16, normalize=True)\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_test)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_test.append(cost_test.ravel())\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_test, 'r')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()    \n",
    "end = time.time()\n",
    "print(end - start)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy. \n",
    "BÃ¥de egen kode og tester med scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.88666666666667 % Training Accuracy\n",
      "81.68 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egen logistisk regresjon med stokastisk gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8413426876068115\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "doplot = False\n",
    "doprint = False\n",
    "Niteration = 100\n",
    "batch_size = 3000\n",
    "batch_size_held_out = nx_train-batch_size\n",
    "\n",
    "beta = np.random.randn(x.shape[1],1)\n",
    "costvec=[]\n",
    "costvec_test=[]\n",
    "loglvec=[]\n",
    "xaxis=[]\n",
    "\n",
    "%matplotlib qt\n",
    "#plt.axis([0, Niteration, 0, 13])\n",
    "\n",
    "indexes = np.arange(nx_train)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(Niteration):\n",
    "    datapoints = np.random.choice(indexes, size=batch_size, replace=False)\n",
    "    batch_x = xtrain[datapoints,:]\n",
    "    batch_y = ytrain[datapoints]\n",
    "    \n",
    "    batch_x_held_out = np.delete(xtrain, datapoints, axis=0)\n",
    "    batch_y_held_out = np.delete(ytrain, datapoints).reshape([batch_size_held_out,1])\n",
    "    \n",
    "    sig = lrf.sigmoid(batch_x@beta)\n",
    "    gradient = lrf.gradient_ols(batch_x,batch_y,sig)\n",
    "    beta -= eta*gradient\n",
    "    \n",
    "    #Cost function\n",
    "    cost = lrf.cost_log_ols(batch_x@beta,batch_y.T)\n",
    "    cost_test = lrf.cost_log_ols(batch_x_held_out@beta,batch_y_held_out.T)\n",
    "    #Log Loss function from sklearn\n",
    "    if doprint:\n",
    "        logloss=log_loss(batch_y, np.round(batch_x@beta), eps=1e-16, normalize=True)\n",
    "        print('Cost', cost,'&','Log loss', logloss,'&','Cost test', cost_test)\n",
    "    if doplot:\n",
    "        costvec.append(cost.ravel())\n",
    "        costvec_test.append(cost_test.ravel())\n",
    "        xaxis.append(iter+1)\n",
    "        plt.plot(xaxis, costvec, 'b')\n",
    "        plt.plot(xaxis, costvec_test, 'r')\n",
    "        plt.pause(1e-12)\n",
    "plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y_held_out.reshape([5000,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.08 % Training Accuracy\n",
      "80.88666666666667 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "activation =lrf.sigmoid(xtrain@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytrain)/len(activation),'% Training Accuracy')\n",
    "\n",
    "\n",
    "activation =lrf.sigmoid(xtest@beta) \n",
    "classes = np.zeros([len(activation)])\n",
    "classes=np.round(activation)\n",
    "print(100*np.sum(classes==ytest)/len(activation),'% Test Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\vemundst\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.19111111111111 % Training Accuracy\n",
      "81.85333333333334 % Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "predicted_classes = model.predict(xtrain)\n",
    "accuracy = accuracy_score(ytrain.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytrain, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Training Accuracy')\n",
    "\n",
    "predicted_classes = model.predict(xtest)\n",
    "accuracy = accuracy_score(ytest.flatten(),predicted_classes)\n",
    "accuracy = accuracy * 100\n",
    "parameters = model.coef_\n",
    "log_loss(ytest, predicted_classes)\n",
    "\n",
    "print(accuracy, '% Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.X_data_full = X_data \n",
    "#self.Y_data_full = Y_data\n",
    "\n",
    "n_inputs = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "\n",
    "#self.epochs = epochs\n",
    "#self.batch_size = batch_size\n",
    "#self.iterations = self.n_inputs // self.batch_size\n",
    "#self.eta = eta\n",
    "#self.lmbd = lmbd\n",
    "\n",
    "\n",
    "\n",
    "n_hidden_neurons=50  # velge andre verdier om vi vil\n",
    "n_categories=2\n",
    "epochs=100\n",
    "batch_size=80\n",
    "eta=0.0001\n",
    "lmbd=0.1\n",
    "\n",
    "\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 50)\n",
      "(50,)\n",
      "(50, 2)\n",
      "(2,)\n",
      "(30000, 91)\n"
     ]
    }
   ],
   "source": [
    "print(hidden_weights.shape)\n",
    "print(hidden_bias.shape)\n",
    "print(output_weights.shape)\n",
    "print(output_bias.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_h = np.matmul(X, hidden_weights) + hidden_bias\n",
    "a_h = lrf.sigmoid(z_h)\n",
    "\n",
    "z_o = np.matmul(a_h, output_weights) + output_bias\n",
    "\n",
    "exp_term = np.exp(z_o)\n",
    "probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 91)\n",
      "(30000, 50)\n",
      "(30000, 50)\n",
      "(30000, 2)\n",
      "(30000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(z_h.shape)\n",
    "print(a_h.shape)\n",
    "print(z_o.shape)\n",
    "print(probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
